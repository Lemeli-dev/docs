---
title: 'LLM Evaluation'
description: 'Evaluate text outputs in under 5 minutes'
noindex: true
mode: "wide"
---

import CloudSignup from '/snippets/cloud_signup.mdx';
import CreateProject from '/snippets/create_project.mdx';

project = ws.create_project("My test project", ##org_id="YOUR_ORG_ID")
project.description = "My project description"
project.save()

<Tabs>
  <Tab title="Evidently Cloud">
    You will run evals locally in Python and send results to Evidently Cloud for analysis and monitoring.

    Need help? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).

    ## 1. Set up Evidently Cloud

    <CloudSignup word="cloud" />

    Now, switch to your Python environment.

    ## 2. Installation and imports

    Install the Evidently Python library:

    ```python
    !pip install evidently[llm]
    ```

    Components to run the evals:

    ```python
    import pandas as pd
    from evidently.report import Report
    from evidently.metric_preset import TextEvals
    from evidently.descriptors import *
    ```

    Components to connect with Evidently Cloud:

    ```python
    from evidently.ui.workspace.cloud import CloudWorkspace
    ```

    ## 3. Create a Project

    <CreateProject word="project" />

    ## 4. Import the toy dataset

    Pass data as a pandas dataframe. Here’s a toy chatbot dataset with "Questions" and "Answers".

    ```python
    data = [
        ["What is the chemical symbol for gold?", "The chemical symbol for gold is Au."],
        ["What is the capital of Japan?", "The capital of Japan is Tokyo."],
        ["Tell me a joke.", "Why don't programmers like nature? It has too many bugs!"],
        ["What is the boiling point of water?", "The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit)."],
        ["Who painted the Mona Lisa?", "Leonardo da Vinci painted the Mona Lisa."],
        ["What’s the fastest animal on land?", "The cheetah is the fastest land animal, capable of running up to 75 miles per hour."],
        ["Can you help me with my math homework?", "I'm sorry, but I can't assist with homework. You might want to consult your teacher for help."],
        ["How many states are there in the USA?", "There are 50 states in the USA."],
        ["What’s the primary function of the heart?", "The primary function of the heart is to pump blood throughout the body."],
        ["Can you tell me the latest stock market trends?", "I'm sorry, but I can't provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor."]
    ]

    columns = ["question", "answer"]

    # Creating the DataFrame
    evaluation_dataset = pd.DataFrame(data, columns=columns)
    ```

    <Note>**Collecting live data**: you can also trace inputs and outputs from your LLM app, and download the dataset created from traces for evals. Check the [Tracing Quickstart](cloud_quickstart_tracing.md). </Note>

    ## 5. Run your first eval

    Let's evaluate all "Answers" for:

    * Sentiment: from -1 for negative to 1 for positive.

    * Text length: character count.

    * Denials: detect if the chatbot denied an answer.

    The last eval uses LLM-as-a-judge with a template Evidently prompt (defaults to `OpenAI` and `gpt-4o-mini`). It returns a label with an explanation.

    <Note>No OpenAI key? We provide an alternative below.</Note>

    Set the OpenAI key as an environment variable. [See Open AI docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).

    ```python
    ## import os
    ## os.environ["OPENAI_API_KEY"] = "YOUR KEY"
    ```

    Run all the evals together:

    ```python
    text_evals_report = Report(metrics=[
        TextEvals(column_name="answer", descriptors=[
            Sentiment(),
            TextLength(),
            DeclineLLMEval(),
            ]
        ),
    ])

    text_evals_report.run(reference_data=None, current_data=evaluation_dataset)
    ```

    **Alternative**. Run a deterministic eval instead of LLM judge: check if words "sorry" or "apologize" are present (True/False).

    ```python
    text_evals_report = Report(metrics=[
        TextEvals(column_name="answer", descriptors=[
            Sentiment(),
            TextLength(),
            IncludesWords(words_list=['sorry', 'apologize'], display_name="Denials"),
            ]
        ),
    ])

    text_evals_report.run(reference_data=None, current_data=evaluation_dataset)
    ```

    Each evaluation is a `descriptor`. You can choose from multiple built-in evals or create a custom ones.

    ## 6. Send results to Evidently Cloud

    **Upload the Report** and include raw data for detailed analysis:

    ```python
    ws.add_report(project.id, text_evals_report, include_data=True)
    ```

    **View the Report**. Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to "Reports" in the left.

    You will see the scores summary, and the dataset with new descriptor columns. For example, you can sort to find all answers with "Denials".

    ![](../.gitbook/assets/cloud/qs_denials.png)

    ## 7. Get a dashboard

    Go to the "Dashboard" tab and enter the "Edit" mode. Add a new tab, and select the "Descriptors" template.

    You'll see a set of panels that show descriptor values. Each has a single data point. As you log ongoing evaluation results, you can track trends and set up alerts.

    ![](../.gitbook/assets/cloud/add_descriptor_tab.gif)

    # What's next?

    Explore the full tutorial for advanced workflows: custom LLM judges, conditional test suites, monitoring, and more.
  </Tab>

  <Tab title="Open-source">
    ## Setup your development - v2

    Learn how to update your docs locally and deploy them to the public.

    ### Edit and preview

    <AccordionGroup>
      <Accordion icon="github" title="Clone your docs locally">
        During the onboarding process, we created a repository on your Github with
        your docs content. You can find this repository on our
        [dashboard](https://dashboard.mintlify.com). To clone the repository
        locally, follow these
        [instructions](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository)
        in your terminal.
      </Accordion>

      <Accordion icon="rectangle-terminal" title="Preview changes">
        Previewing helps you make sure your changes look as intended. We built a
        command line interface to render these changes locally. 1. Install the
        [Mintlify CLI](https://www.npmjs.com/package/mintlify) to preview the
        documentation changes locally with this command: `npm i -g mintlify`
        2\. Run the following command at the root of your documentation (where
        `mint.json` is): `mintlify dev`
      </Accordion>
    </AccordionGroup>

    ### Deploy your changes

    <AccordionGroup>
      <Accordion icon="message-bot" title="Install our Github app">
        Our Github app automatically deploys your changes to your docs site, so you
        don't need to manage deployments yourself. You can find the link to install on
        your [dashboard](https://dashboard.mintlify.com). Once the bot has been
        successfully installed, there should be a check mark next to the commit hash
        of the repo.
      </Accordion>

      <Accordion icon="rocket" title="Push your changes">
        [Commit and push your changes to
        Git](https://docs.github.com/en/get-started/using-git/pushing-commits-to-a-remote-repository#about-git-push)
        for your changes to update in your docs site. If you push and don't see that
        the Github app successfully deployed your changes, you can also manually
        update your docs through our [dashboard](https://dashboard.mintlify.com).
      </Accordion>
    </AccordionGroup>

    ## Update your docs

    Add content directly in your files with MDX syntax and React components. You can use any of our components, or even build your own.

    <CardGroup>
      <Card title="Style Your Docs" icon="paintbrush" href="/settings/global">
        Add flair to your docs with personalized branding.
      </Card>

      <Card title="Add API Endpoints" icon="square-code" href="/api-playground/configuration">
        Implement your OpenAPI spec and enable API user interaction.
      </Card>

      <Card title="Integrate Analytics" icon="chart-mixed" href="/analytics/supported-integrations">
        Draw insights from user interactions with your documentation.
      </Card>

      <Card title="Host on a Custom Domain" icon="browser" href="/settings/custom-domain/subdomain">
        Keep your docs on your own website's subdomain.
      </Card>
    </CardGroup>
  </Tab>
</Tabs>