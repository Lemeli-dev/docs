---
title: 'Run evals via API'
description: 'How to run evals and log them on the platform'
noindex: true
---

![](/images/evals_flow_python.png)

<Tip>
  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).&#x20;
</Tip>

<Steps>
  <Step title="Configure evaluations">
    To run evals, use the basic flow of the Evidently evaluation library. For comparative evals, use Reports. For regression testing or other testing scenarios, add Tests conditions. Check the Quickstart examples or explore the [detailed guide.](/docs/library/evaluations_overview)

    <CardGroup cols={2}>
      <Card title="LLM quickstart" icon="comment-text" href="/quickstart_llm">
        Evaluate the quality of text outputs.
      </Card>

      <Card title="ML quickstart" icon="table" href="/quickstart_ml">
        Test tabular data quality and data drift.
      </Card>
    </CardGroup>

    You can choose to upload raw data only the evaluation results.
  </Step>

  <Step title="Explore the results">
    Once you run the evaluation, go to the Explore view inside your Project to debug the individual Results and compare the outcomes between runs. Understand the [Explore view](/docs/platform/evals_explore).
  </Step>

  <Step title="(Optional) Set up a Dashboard">
    Optionally, configure a Dashboard to track results over time. This helps you monitor metric changes across experiments to see your progress or results of ongoing safety Tests. Check the docs on [Dashboard](/docs/platform/dashboard_overview).
  </Step>

  <Step title="(Optional) Configure alerts">
    Optionally, configure alerts on failed Tests. Check the section on [Alerts](/docs/platform/alerts).
  </Step>
</Steps>

## Example

A simple example of a single eval with dataset summary uploaded to a workspace:

```
eval_data = Dataset.from_pandas(
    pd.DataFrame(source_df),
    data_definition=DataDefinition()
)

report = Report([
    DatasetStats()
])

my_eval = report.run(eval_data, None)
ws.add_report(project.id, my_eval, include_data=False)
```

## Upload data

<Info>
  Raw data upload is available only for Evidently Cloud and Enterprise.
</Info>

When you upload a Report, you can decide to:

* include only the resulting Metrics and a summary Report (with distribution summaries, etc.), or

* also upload the raw Dataset you evaluated, together with added Descriptors (if any). This helps with row-level debugging and analysis.

Use`include_data` (default `False`) to specify whether to include the data.

```python
ws.add_report(project.id, my_eval, include_data=False)
```

## Upload existing snapshots

The `add_report` method save Reports in the correct JSON format (`snapshot`). If you already have a snapshot (e.g., you previously saved a Report as a snapshot locally), you can load it load and send it to your Project:

```python
ws.add_snapshot(project.id, snapshot.load("data_drift_snapshot.json"))
```

## Delete snapshots

To delete individual snapshots in the Workspace `ws`, pass the Project ID and snapshot ID. You can see the snapshot ID on the Report page.&#x20;

```python
ws.delete_snapshot(project_id, snapshot_id)
```

You can also delete Reports from the UI.