---
title: 'Overview'
description: 'Description of your new file.'
noindex: true
---

## What are AI quality evals

You may need evaluations at different stages of your AI product development:

* **Ad hoc analysis.** Spot-check the quality of your data or AI outputs when new data arrives or during troubleshooting.

* **Experiments**. During development, test different parameters, models, or prompts, and compare outcomes side-by-side to track improvements on your test set.

* **Safety and adversarial testing.** Before deployment, evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.

* **Regression testing.** Ensure performance does not degrade after updates or fixes, often as part of CI/CD pipelines.

Evidently supports all these workflows.

## Evaluations via API

<Check>
  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.
</Check>

This is perfect for experiments, CI/CD workflows, or custom evaluation pipelines.

![](/images/evals_flow_python.png)

**How it works**:

* Run Python-based evaluations on your AI outputs by generating Reports.

* Upload results to the Evidently Platform, including only metrics and/or raw datasets.

* Use the Explore feature to compare and debug results between runs.

**Next step:** Quickstart for [ML](/ml_quickstart) or [LLM](/llm_quickstart).

## No-code evaluations

<Check>
  Supported in `Evidently Cloud` and `Evidently Enterprise`.
</Check>

This option lets you run evaluations directly in the user interface. This is great for non-technical users or when you prefer to run evaluations on Evidently infrastructure.

![](/images/evals_flow_nocode.png)

**How it works**:

* **Analyze CSV datasets**. Drag and drop CSV files and evaluate their contents on the Platform.

* **Evaluate uploaded datasets**. Assess collected [traces](/docs/platform/tracing_overview) from instrumented LLM applications or any [Datasets](/docs/platform/datasets_overview) you previously uploaded or generated.

No-code workflows create the same Reports or Test Suites you'd generate using Python. The rest of the workflow is the same. After you run your evals with any method, you can access the results in the Explore view for further analysis.