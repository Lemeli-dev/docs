---
title: "Descriptors"
description: "How to run evaluations for text data."
noindex: "true"
---

For a general introduction, check [Core Concepts](/docs/library/overview).

## Generate descriptors

### Imports

```python
from evidently.future.datasets import Dataset
from evidently.future.datasets import DataDefinition
from evidently.future.datasets import Descriptor
from evidently.future.descriptors import *
```

**Note**. Some Descriptors that use vocabulary-based checks (like `OOVWordsPercentage()` for out-of-vocabulary words) require downloading `nltk` dictionaries:

```python
nltk.download('words')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('vader_lexicon')
```

### Basic flow

<Accordion title="Generate data" defaultOpen={false}>
  Use the following code to generate toy data for this guide

  ```python
  import pandas as pd

  data = [
      ["What is the chemical symbol for gold?", "The chemical symbol for gold is Au."],
      ["What is the capital of Japan?", "The capital of Japan is Tokyo."],
      ["Tell me a joke.", "Why don't programmers like nature? It has too many bugs!"],
      ["What is the boiling point of water?", "The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit)."],
      ["Who painted the Mona Lisa?", "Leonardo da Vinci painted the Mona Lisa."],
      ["What’s the fastest animal on land?", "The cheetah is the fastest land animal, capable of running up to 75 miles per hour."],
      ["Can you help me with my math homework?", "I'm sorry, but I can't assist with homework. You might want to consult your teacher for help."],
      ["How many states are there in the USA?", "There are 50 states in the USA."],
      ["What’s the primary function of the heart?", "The primary function of the heart is to pump blood throughout the body."],
      ["Can you tell me the latest stock market trends?", "I'm sorry, but I can't provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor."]
  ]

  # Columns
  columns = ["question", "answer"]

  # Creating the DataFrame
  df = pd.DataFrame(data, columns=columns)
  ```
</Accordion>

If you have a dataframe `df` with text data, you can create a `Dataset`, set the [Data Definition](/docs/library/data_definition) and add descriptors for the "answer" column:

```python
eval_dataset = Dataset.from_pandas(
    pd.DataFrame(df),
    data_definition=DataDefinition(
        text_columns=["question", "answer"]),
    descriptors=[
        Sentiment("answer", alias="Sentiment"),
        TextLength("answer", alias="Length"),
        IncludesWords("answer", words_list=['sorry', 'apologize'], alias="Denials"),
    ]
)
```

You can also add descriptors to the Dataset object later using `add_descriptors`. For example, first set the data schema:

```python
eval_dataset = Dataset.from_pandas(
    pd.DataFrame(df),
    data_definition=DataDefinition(text_columns=["question", "answer"]),
)
```

Then, add the scores to this Dataset:

```python
eval_dataset.add_descriptors(descriptors=[
    Sentiment("answer", alias="Sentiment"),
    TextLength("answer", alias="Length"),
    IncludesWords("answer", words_list=['sorry', 'apologize'], alias="Denials"),
])
```

You can view the Dataset with newly added descriptors as a pandas DataFrame:

```python
eval_dataset.as_dataframe()
```

### Customization

<Tip>
  **All descriptors and parameters**. See a [reference table](/metrics/all_descriptors) with all descriptors and parameters.
</Tip>

**Alias**. It is best to add an `alias` to each Descriptor to make it easier to reference. This name shows up in visualizations and column headers. It’s especially handy if you’re using checks like regular expressions with word lists, where the auto-generated title could get very long.

```python
eval_dataset.add_descriptors(descriptors=[
    WordCount("answer", alias="Words"),
])
```

**Descriptor parameters**. Some Descriptors have required parameters. For example, if you’re testing for competitor mentions using the `Contains` Descriptor, you must include the list of `items`:

```python
eval_dataset.add_descriptors(descriptors=[
    Contains("answer", items=["AcmeCorp", "YetAnotherCorp"], alias="Competitors")
])
```

**Multi-column descriptors**. Some evals use more than one column. For example, to compare a new answer match against reference answer, or measure semantic similarity. In this case, pass both columns using parameters:

```python
eval_dataset.add_descriptors(descriptors=[
    SemanticSimilarity(columns=["question", "answer"], alias="Semantic_Match")
])
```

**LLM-as-a-judge**. There are also built-in descriptors that prompt an external LLM to return an evaluation score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.


```python
eval_dataset.add_descriptors(descriptors=[
    DeclineLLMEval("answer", alias="Contains_Denial")
])
```

<Note>
  **Custom LLM judges**. Check a [separate guide](/metrics/customize_llm_judge) on how to use built-in LLM judges and create custom evaluators for your criteria.
</Note>

<Note>
  **Custom descriptors**. You can also implement custom programmatic evaluations as functions. Check a [separate guide](/metrics/customize_descriptor).
</Note>

## Get a Report

Once you added descriptors to the Dataset, you can summarize the results using Reports. This lets you get a summary score for all descriptors, visualize their distributions and run conditional tests.

### Imports

```python
from evidently.future.report import Report
from evidently.future.presets import TextEvals
from evidently.future.metrics import *
from evidently.future.tests import *
```

### Text Evals

The easiest way to get the Report is through `TextEvals` Preset: it instantly summarizes all Descriptor values for a specific column.

To configure the Report and run it for `eval_dataset`

```python
report = Report([
    TextEvals()
])
my_eval = report.run(eval_dataset, None)
```

You can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).

```python
my_eval
# my_eval.json()
# ws.add_report(project.id, my_eval, include_data=True)
```

### Using Metrics

The `TextEvals` Preset works by generating a `ValueStats` Metrics for each Descriptor. If you want more control or to use other Metrics available in Evidently, you can create a custom Report referencing descriptors just like any other column in the dataset.

**Custom Report**. For example, you can visualize the mean values of descriptors:

```python
custom_report = Report([
    MeanValue(column="Length"),
    MeanValue(column="Sentiment")
])

my_custom_eval = custom_report.run(eval_dataset, None)
my_custom_eval.json()
```

**Drift detection**. You can also run more complex checks like drift detection, such as comparing the distribution of text length between two batches of data. (This requires two datasets).

```python
custom_report = Report([
    ValueDrift(column="Length"),
])

my_custom_eval = custom_report.run(eval_dataset, eval_dataset)
my_custom_eval.json()
```

### Run Tests

You can add test conditions to Metrics to obtain Pass/Fail results. For example:
* Test that no response has a negative Sentiment (lower than 0).
* Test that no response has a Length of over 150 symbols.

```python
tests = Report([
    MinValue(column="Sentiment", tests=[gte(0)]),
    MaxValue(column="Length", tests=[lte(150)]),
])

my_test_eval = tests.run(eval_dataset, None)
my_test_eval
# my_test_eval.json()
```

This adds a Test Suite to the Report for clear pass/fail outcomes.

You can use different Tests depending on the column you are Testing. For example, to check that the chatbot does not deny an answer, you can test `CategoryCount` (check how many `True` labels are in the column with the regex check).

```python
from evidently.future.metrics.column_statistics import CategoryCount

tests = Report([
    CategoryCount(column="Denials", category=True, count_tests=[eq(0)])
])

my_test_eval = tests.run(eval_dataset, None)
my_test_eval
# my_test_eval.json()
```

<Note>
  **Report and Tests API**. Check separate guides on [generating Reports](/docs/library/reports) and setting [Test conditions](/docs/library/tests).
</Note>

<Note>
  **List of all Metrics**. To find specific Metrics, check the [Reference table](/metrics/all_metrics). You can use any column-level Metrics for computed Descriptors, like `MeanValue`, `MeanValue`, `MaxValue`, `QuantileValue`, `OutRangeValueCount() for numerical and `CategoryCount` for categorical.
</Note>
