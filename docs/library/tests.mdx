---
title: 'Tests'
description: 'How to run conditional checks.'
noindex: true
---

For a general introduction, check [Core Concepts](/docs/library/overview).

**Pre-requisites**:

* You know how to [generate Reports and select Metrics](/docs/library/report).

## Example

Check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm) for a simple example of running Tests.

## Imports

In addition to Metrics or Presets, also import `tests`.

```python
from evidently.future.report import Report
from evidently.future.metrics import *
from evidently.future.presets import *
from evidently.future.tests import *
```

## Running Tests

You add Tests by configuring additional parameters on the Report or Metrics level. There are 3 ways to run Tests:

* **Tests Presets**. Get a suite of pre-selected Tests with auto-generated conditions. No setup required.

* **Individual Test with defaults**. You can pick Tests one by one, but auto-generate conditions.

* **Fully custom**. You can choose Tests and set conditions manually.

Evidently has very flexible API, which makes it easy to formulate nuanced conditions and run Tests for lots of columns at once.

## Test Presets

Test Presets are pre-built Test Suites that generate Tests for a specific aspect of the data or AI system performance.

You call a Test Preset by enabling `include_tests=True` Report option. This will generate a set of associated Tests:

```python
report = Report([
    DataSummaryPreset(),
],
include_tests=True)
```

For example, while `DataSummaryPreset()` itself shows descriptive stats of the data, the associated Test Suite will run multiple checks on data quality and expected feature stats for all metrics inside the preset.

**Based on reference**. Provide the reference Daatset as you run the Report:

```Python
my_eval = report.run(eval_data_1, eval_data_2)
```

In this case, Evidently will derive conditions from this reference. For example, it will derive the reference share of missing values in each column and Test that the current share of missing values is within +/-10% of that. 10% is an encoded heuristic.

**Based on heuristics**. Without a reference, Evidently uses direct heuristics.

```Python
my_eval = report.run(eval_data_1, None)
```

In this case, Test on share of missing values will assume that it should be 0. This is a fixed heuristic. Similarly, Test on `Accuracy()` inside the `ClassificationPreset` will fail if the model performs worse than a dummy model created by Evidently. When it's not practical, some checks (like check on min/max/mean value) don't have an heuristics-based condition.

By selecting specific columns for the Preset, you reduce the number of generated column-level Tests.

<Info>
  **How to check Test defaults?** Consult the [All Metrics](/metrics/all_metrics) reference table.
</Info>

## Individual Tests with defaults

Presets are great for a start or quick sanity checks, but in most cases you'd want to select specific Tests to run to minimize their overall number. You can still use the default Test conditions.

**Select Tests**. Simply pass the Metrics to the Report and choose the include Tests option for the Report to enable Tests for these Metrics.

```Python
report = Report([
    MissingValueCount(column="Age"),
    MinValue(column="Age"),
], 
include_tests=True)
```

If you run this Report passing two datasets, it will derive Test conditions from reference. If you pass only one, it will use heuristics.

**Exclude some Tests**. If you want to exclude generating the Test for some Metrics / Presets, use the `tests` parameter and keep it as None or empty.

```Python
report = Report([
    MissingValueCount(column="Age", tests=[]),
    MinValue(column="Age"),
], 
include_tests=True)
```

This will only generate the Test on `MinValue()`.

## Custom Test conditions

You can also define when exactly a Test should pass or fail. For example, set a lower boundary for the expected model precision, or share of drift columns. If the condition is violated, the Test fails.

Use parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. to set the expected behavior.

**Custom conditions**. For example, to test that there are no missing values in column "Age" and minimal value is greater or equal to 18:

```Python
report = Report([
    MissingValueCount(column="Age", tests=[eq(0)]),
    MinValue(column="Age", tests=[gte(18)]),
])
```

Notice that in this case we do not pass `include_tests` since (it defaults to `False`) since we set all Tests manually.

Here are conditions you can set:

| Condition      | Explanation                                       | Example                                                |
| -------------- | ------------------------------------------------- | ------------------------------------------------------ |
| `eq(val)`      | equal to <br /> `test_result == val` <br /><br /> Accepts `epsilon` parameter to set a fuzzy +/- range. Useful for float values.             | `MinValue(column="Age", tests=[eq(18)])` <br /> `MinValue(column="Score", tests=[eq(1, epsilon=0.01])`      )           |
| `not_eq(val)`  | not equal <br /> `test_result != val`             | `MinValue(column="Age", tests=[not_eq(18)])`           |
| `gt(val)`      | greater than  <br /> `test_result > val`          | `MinValue(column="Age", tests=[gt(18)])`               |
| `gte(val)`     | greater than or equal <br /> `test_result >= val` | `MinValue(column="Age", tests=[gte(18)])`              |
| `lt(val)`      | less than <br /> `test_result < val`              | `MinValue(column="Age", tests=[lt(18)])`               |
| `lte(val)`     | less than or equal <br /> `test_result <= val`    | `MinValue(column="Age", tests=[lte(18)])`              |
| `is_in: list`  | `test_result ==` one of the values                | `MinValue(column="Age", tests=[is_in([18, 21, 30])])`  |
| `not_in: list` | `test_result !=` any of the values                | `MinValue(column="Age", tests=[not_in([16, 17, 18])])` |


**Additional parameters**. Some Metrics require additional parameters. This helps simplify setting nuanced conditions. 

For example, to check there there are no value out of 18-80 range:

```
report = Report([
    OutRangeValueCount(column="Age", left=18, right=80, tests=[eq(0)]),
])  
```

**Combine custom and default conditions**. You can also use defaults for some Tests and set custom conditions for others. Set `include_tests` as True and add custom conditions where you want:

```Python
report = Report([
    RowCount(tests=[gt(10)]),
    MissingValueCount(column="Age"),
],
include_tests=True) 
```

This will override default conditions for the Tests where you added yours.

**Multiple conditions**. You can simultaneoulsy add multiple conditions to the same Metric. For example:

```
report = Report([
    MinValue(column="Age", tests=[gte(17), lte(19)]),
])
```

This will generate two Tests for the same value. 

**Share Tests**. Most of the Metrics expect `tests` parameters to define a condition. However, some Metrics return two metric results: `count` and `share`. For example, the `MissingValueCount` or `CategoryCount` return both absolute value and share. If you want to Test share, you must use `share_tests` instead.

For example, you can check if there are less than 5 missing values (absolute value):

```
report = Report([
    MissingValueCount(column="Age", count_tests=[lte(5)]), #replace for tests
])
```

Or less than 10% missing values (relative):

```
report = Report([
    MissingValueCount(column="Age", share_tests=[lte(0.1)]),
])
```

**Set custom conditions relative to reference**. You can also set custom conditions by specifying relative or absolute difference to the reference value. This requires you to provide the reference dataset.

For example, to check that the number of rows in the new dataset is equal or greater by up to 10% from the referene.

```
from evidently.future.metric_types import Reference
report = Report([
   RowCount(tests=[gte(Reference(relative=0.1))]),
])

my_eval = report.run(eval_data_1, eval_data_2)
```

You can also set absolute conditions in relation to the reference. This will check that 

```
report = Report([
   RowCount(tests=[gte(Reference(absolute=5))]),
])
```

**(TBC) Custom conditions relative to a value**.


## Test criticality

By default, all Tests will return a `Fail` if the Test condition is not fulfilled. If you want to get a `Warning` instead, use the `is_critical` parameter and set it to `False`. 

For example: 

```
report = Report([
    MissingValueCount(column="Age", share_tests=[eq(0, is_critical=False)]),
])
```
Failed Tests will be labeled as Warning and not trigger alerts (if alerts are enabled on failed Tests).

You can use this to set "layered" conditions. For example, if the share of missing values is non-zero, you get a Warning. If it's higher than 10%, get an Alert.

```python
report = Report([
    MissingValueCount(column="Age", 
                      share_tests=[eq(0, is_critical=False), 
                                   lte(0.1, is_critical=True)]),
])

my_eval = report.run(eval_data_1, None)
my_eval
```

