---
title: 'Report'
description: 'How to generate Report.'
noindex: true
---

For a general introduction, check [Core Concepts](/docs/library/overview).

Pre-requisites:

* You installed Evidently.

* You created a Dataset with the Data Definition.

* (Optional) for text data, you added Descriptors.

## Generate Reports

Check the [Quickstart](/quickstart_ml) for a simple example.

### Imports

Pick the required Metrics and Presets you plan to use.

```python
from evidently.future.report import Report
from evidently.future.metrics import *
from evidently.future.presets import *
```

You can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one.

### Use Presets

<Tip>
  **Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).
</Tip>

To generate a template Report, simply pass the selected Preset in the `metrics` list.

**Data Summary**. To generate the Data Summary Report for a single dataset:

```python
report = Report([
    DataSummaryPreset()
])

my_eval = report.run(eval_data_1, None)
my_eval
#my_eval.json
```

**Data Drift**. To generate the Data Drift Report for two datasets (pass the second one as a reference):

```python
report = Report([
    DataDriftPreset()
])

my_eval = report.run(eval_data_1, eval_data_2)
my_eval
#my_eval.json
```

If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. You can also pass custom parameters to some Presets.

**Combine Presets**. You can also include multiple Presets in the same Report.

```python
report = Report([
    DataDriftPreset(), 
    DataSummaryPreset()
])

my_eval = report.run(eval_data_1, eval_data_2)
my_eval
#my_eval.json
```

**Limit columns**. You can limit the columns to which the Preset is applied.

```python
report = Report([
    DataDriftPreset(column=["target", "prediction"])
])

my_eval = report.run(eval_data_1, eval_data_2)
my_eval
#my_eval.json
```

You can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).

### Custom Report 

<Tip>
  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).
</Tip>

**Custom Report**. To create a custom Report, simply list the `metrics` one by one. You can combine both dataset-level and column-level Metrics, and combine Reports and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.

```python
report = Report([
    ColumnCount(), 
    ValueStats(column="target")
])

my_eval = report.run(eval_data_1, None)
my_eval
#my_eval.json
```

<Note>
**Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).
</Note>

**Metric Parameters**. Metrics can have optional or required parameters. 

For example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional). 

```python
report = Report([
   ValueDrift(column="target", method="psi")
])
```

To calculate the Precision at K for a ranking task, you must always pass the `k` parameter (Required).

```python
report = Report([
   PrecisionTopK(k=10)
])
```

### Tests 

You can also add conditions to Metrics: check the [Tests guide](/docs/library/tests).