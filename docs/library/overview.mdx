---
title: "Overview"
description: "Core concepts and components of the Evidently Python library."
noindex: "true"
---

The Evidently Python library is an open-source tool designed to evaluate, test and monitor the quality of AI systems, from experimentation to production.

Evidently has a modular architecture. You can use the library as a standalone evaluation tool, or together with the Monitoring Platform (self-hosted or Evidently Cloud) to track evals over time.

This page gives a closer look at the library. For a Platform overview, [head here](/docs/platform/overview).

## At a glance

\[ADD IMG]

Here is how the core evaluation workflow works in Evidently:

<Steps>
  <Step title="Get input data">
    Pass a dataset with text, tabular or embeddings data. Optionally include the reference dataset for comparison.
  </Step>

  <Step title="Run evaluations">
    Pick from 100+ built-in or add custom metrics. Optionally, set test conditions. Here are some types of evals implemented in the library.

| **Type**                                                 | **Example Checks**                                                                       |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **üî° Text descriptors**                                   | Length, sentiment, toxicity, language, special symbols, regular expression matches, etc. |
| **üìù LLM output quality**                                 | Semantic similarity, response relevance, faithfulness, custom LLM judges, etc.           |
| **üõ¢ Data quality**                                       | Missing values, duplicates, min-max ranges, new categorical values, correlations, etc.   |
| **üìä Data distribution drift**                            | 20+ statistical tests and distance metrics to compare shifts in data distribution.       |
| **üéØ Classification**                                     | Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.                       |
| **üìà Regression**                                         | MAE, ME, RMSE, error distribution, error normality, error bias, etc.                     |
| **üóÇ Ranking (incl. RAG)**                                | NDCG, MAP, MRR, Hit Rate, etc.                                                           |
| **üõí Recommendations**                                    | Serendipity, novelty, diversity, popularity bias, etc.                                   |

  </Step>

  <Step title="Explore results">
    * View visual Reports directly in Python environments like Jupyter or Colab.

    * Export as JSON, Python dictionary, pandas DataFrame, or HTML file.

    * Upload to Evidently Platform to store and track over time.
  </Step>
</Steps>

There are 3 core components you may need to run evals: Descriptors, Metrics and Tests.

## Descriptors

To evaluate text data like LLM outputs, you'll need `Descriptors`. These are row-level scores or labels that assess specific qualities of text. It‚Äôs different from metrics like accuracy which give one score for an entire dataset.

You can use built-in descriptors, configurable templates (like LLM judges), or add yours.

Descriptors let you evaluate anything from TextLength, Sentiment, or whether the text contains personal data. They can also compare two texts, such as checking SemanticSimilarity between two columns. Each Descriptor returns a result that can be:

* **Numerical**. Any scores like symbol count or cosine similarity.

* **Categorical**. Labels or binary ‚Äútrue‚Äù/‚Äúfalse‚Äù results for pattern matches.

* **Text string**. Like explanations generated by LLM.

Evidently adds the computed descriptor values directly to the dataset. This helps with debugging: for example, you can sort to find the negative responses. You can export the results to a Pandas DataFrame or view on the Evidently Platform.![](/images/concepts/overview_descriptors_export.png)

After you get the Descriptors, you can use them to compute Metrics and Tests.

## Reports and Metrics

### What is a Metric?

A `Metric` is a key building block for evaluations at the **dataset** or **column** level. Each Metric computes a single value and has an optional visual representation (or several to choose from).

**Using Metrics with Descriptors.** After computing row-level Descriptors, you can use Metrics to aggregate the results across the dataset. For instance, you can calculate the `ValueMean` for sentiment or similarity scores. Descriptors are treated like any other column.

**Using Metrics directly.** You can also compute Metrics directly on the data you have, like a table with ML model logs. You can use Metrics both to understand your data (like column statistics) and to evaluate different aspects of AI quality (like response accuracy).

Metrics range from simple stats like row number to complex algorithmic and prompt-based evals. Some examples:

<Tabs>
  <Tab title="ValueMean">
    Compute the mean value of any given column. \[ADD IMG]

    ![](/images/concepts/overview_small_preset_cat_value_example.png)
  </Tab>

  <Tab title="ValueDrift">
    Compute drift score for any individual column, target or prediction. You can set drift detection methods as a parameter.
  </Tab>

  <Tab title="Accuracy">
    Compute classification accuracy.&#x20;
  </Tab>

  <Tab title="MissingValues">
    Compute the share of missing values in the dataset.
  </Tab>

  <Tab title="Diversity">
    Compute the diversity of generated recommendations.
  </Tab>

  <Tab title="MeanError">
    Compute mean error for regression tasks.
  </Tab>
</Tabs>

You can combine multiple individual Metrics in a Report.¬†

<Card title="Explore evaluations" icon="chart-simple" horizontal="horizontal" href="/metrics">
  See all metrics and how to customize.
</Card>