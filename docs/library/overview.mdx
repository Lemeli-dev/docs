---
title: "Overview"
description: "Evidently Python library at a glance. "
noindex: "true"
---

Evidently library simplifies quality evaluations for AI systems. It supports both predictive ML models and generative AI systems, from classification to RAG.

You can use Evidently as a standalone open-source tool or as part of the Evidently Platform.

\[ADD IMG]

## Workflow

<Steps>
  <Step title="Pass input data">
    Use text, tabular, or embeddings data. Optionally [define a schema](/docs/library/data_overview).
  </Step>

  <Step title="Configure evaluations">
    Evidently supports [3 evaluation modes](/docs/library/evaluations_overview) with a declarative API.

    * `Descriptors` compute row-level scores for individual inputs.
    * `Metrics` compute and aggregate dataset scores.
    * `Tests` add pass/fail conditions to metrics. You can set conditions manually or automatically infer from the reference dataset, like min-max ranges.

    Pick from 100+ metrics or add yours.
  </Step>

  <Step title="Get results">
    Choose how to get the outputs.&#x20;

    * **View the Report** in an interactive Python environment (Jupyter, Colab).
    * **Export** as JSON, Python dictionary, pandas DataFrame, or HTML file.
    * **Upload** to the Evidently Platform to store and track over time.
  </Step>
</Steps>

## Available evaluations&#x20;

Evaluations are a core part of the library. Evidently has 100+ built-in evals, and Presets that group popular checks together.

Each evaluation has optional **interactive visuals** to explore results, such as feature or score distributions.

Here are examples of things you can check:

|                                                                                          |                                                                                                        |
| :--------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: |
|                                  **üî° Text descriptors**                                 |                                           **üìù LLM outputs**                                           |
| Length, sentiment, toxicity, language, special symbols, regular expression matches, etc. | Semantic similarity, retrieval relevance, summarization quality, etc. with model- and LLM-based evals. |
|                                    **üõ¢ Data quality**                                   |                                     **üìä Data distribution drift**                                     |
|  Missing values, duplicates, min-max ranges, new categorical values, correlations, etc.  |           20+ statistical tests and distance metrics to compare shifts in data distribution.           |
|                                   **üéØ Classification**                                  |                                            **üìà Regression**                                           |
|            Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.            |                  MAE, ME, RMSE, error distribution, error normality, error bias, etc.                  |
|                                 **üóÇ Ranking (inc. RAG)**                                |                                         **üõí Recommendations**                                         |
|                              NDCG, MAP, MRR, Hit Rate, etc.                              |                         Serendipity, novelty, diversity, popularity bias, etc.                         |

You can also implement custom checks in Python or define your prompts for LLM-as-a-judge.

<Card title="Explore evaluations" icon="chart-simple" horizontal="horizontal" href="/metrics">
  See all metrics and how to customize.
</Card>