---
title: "Overview"
description: "Evidently Python library at a glance. "
noindex: "true"
---

Evidently library simplifies quality evaluations for AI systems. It supports both predictive ML models and generative AI systems, from classification to RAG.

You can use Evidently as a standalone open-source tool or as part of the Evidently Platform.

\[ADD IMG]


## Available evaluations

Evaluations are a core part of the library. Evidently has 100+ built-in evals, and Presets that group popular checks together. Each evaluation has an optional **interactive visualization** to help explore and interpret the results.

Here are examples of things you can check:

|                                                                                          |                                                                                                        |
| :--------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: |
|                                  **üî° Text descriptors**                                 |                                           **üìù LLM outputs**                                           |
| Length, sentiment, toxicity, language, special symbols, regular expression matches, etc. | Semantic similarity, retrieval relevance, summarization quality, etc. with model- and LLM-based evals. |
|                                    **üõ¢ Data quality**                                   |                                     **üìä Data distribution drift**                                     |
|  Missing values, duplicates, min-max ranges, new categorical values, correlations, etc.  |           20+ statistical tests and distance metrics to compare shifts in data distribution.           |
|                                   **üéØ Classification**                                  |                                            **üìà Regression**                                           |
|            Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.            |                  MAE, ME, RMSE, error distribution, error normality, error bias, etc.                  |
|                                 **üóÇ Ranking (inc. RAG)**                                |                                         **üõí Recommendations**                                         |
|                              NDCG, MAP, MRR, Hit Rate, etc.                              |                         Serendipity, novelty, diversity, popularity bias, etc.                         |

You can also implement custom checks in Python or define your prompts for LLM-as-a-judge.

<Card title="Explore evaluations" icon="chart-simple" horizontal="horizontal" href="/metrics">
  See all metrics and how to customize.
</Card>