---
title: "Overview"
description: "Core concepts and components of the Evidently Python library."
noindex: "true"
---

The Evidently Python library is an open-source tool designed to evaluate, test and monitor the quality of AI systems, from experimentation to production.

Evidently has a modular architecture. You can use the evaluation library on its own, or together with the Monitoring Platform (self-hosted or Evidently Cloud).

This page gives a closer look at the library. For a Platform overview, [head here](/docs/platform/overview).

# At a glance

\[ADD IMG]

Here is how the core evaluation workflow works in Evidently:

<Steps>
  <Step title="Pass the input data">
    Pass a dataset with text, tabular or embeddings data. Optionally include a second reference dataset for comparison.
  </Step>

  <Step title="Run evaluations">
    For a quick start, you can use evaluation `Presets`. Alternatively, create a custom evaluation setup. You can compute row-level `Descriptors` for text data and use `Metrics` for dataset-level evals. Optionally, add `Tests` to get pass/fail results.

    There are 100+ built-in evals. You can also add yours. Examples:

    | **Type**                  | **Example checks**                                                         |
    | ------------------------- | -------------------------------------------------------------------------- |
    | **üî° Text qualities**     | Length, sentiment, special symbols, pattern  matches, etc.                 |
    | **üìù LLM outputs**        | Semantic similarity, relevance, faithfulness, custom LLM judges, etc.      |
    | **üõ¢ Data quality**       | Missing values, duplicates, min-max ranges, new values, correlations, etc. |
    | **üìä Data drift**         | 20+ tests and distance metrics to detect distribution drift.               |
    | **üéØ Classification**     | Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.         |
    | **üìà Regression**         | MAE, ME, RMSE, error distribution, error normality, error bias, etc.       |
    | **üóÇ Ranking (inc. RAG)** | NDCG, MAP, MRR, Hit Rate, etc.                                             |
    | **üõí Recommendations**    | Serendipity, novelty, diversity, popularity bias, etc.                     |
  </Step>

  <Step title="Explore results">
    * View visual Reports directly in Python environments like Jupyter or Colab.

    * Export as JSON, Python dictionary, pandas DataFrame, or HTML file.

    * Upload to Evidently Platform to store and track over time.
  </Step>
</Steps>

# Core concepts

Below is a conceptual explanation of how any evaluatons work in Evidently. For a practical code example:
* Check the quickstarts for [LLM evaluation](quickstart_llm) or [ML evaluation](quickstart_ml).
* Read the [user guide](/docs/library/evaluations_overview).

## Input data

**Tabular data**. To run an evaluation, you must prepare your data in tabular format as a pandas DataFrame. All column names must be strings.

**Flexible structure**. The table can include any combination of numerical, categorical, text, metadata (including timestamps or IDs), and embedding columns. Here are a few examples.

<Tabs>
  <Tab title="LLM logs">
    To evaluate LLM outputs, pass any text columns with optional context, ground truth and metadata.

    | Question                             | Context                                                                                                   | Answer                          |
    | ------------------------------------ | --------------------------------------------------------------------------------------------------------- | ------------------------------- |
    | How old is the universe?             | The universe is believed to have originated from the Big Bang that occurred 13.8 billion years ago.       | 13.8 billion years old.         |
    | What‚Äôs the lifespan of Baobab trees? | Baobab trees can live up to 2,500 years. They are often called the ‚ÄúTree of Life‚Äù.                        | Up to 2,500 years.              |
    | What is the speed of light?          | The speed of light in a vacuum is approximately 299,792 kilometers per second (186,282 miles per second). | Close to 299,792 km per second. |
  </Tab>

  <Tab title="Data table">
    Pass any data table to get a data overview, run data quality and distribution drift checks.

    These can also be production logs of your ML model without ground truth: include input features and predicted values.

    | Order ID | Product                | Category    | Quantity | Price  | Payment Method | Shipping Status |
    | -------- | ---------------------- | ----------- | -------- | ------ | -------------- | --------------- |
    | ORD001   | Wireless Headphones    | Electronics | 1        | 120.00 | Credit Card    | Shipped         |
    | ORD002   | Yoga Mat               | Sports      | 2        | 45.00  | PayPal         | In Transit      |
    | ORD003   | Stainless Steel Bottle | Kitchen     | 3        | 30.00  | Debit Card     | Delivered       |
  </Tab>

  <Tab title="Classification">
    To evaluate classification quality, pass a table that contains columns with predicted and actual labels. Input features are optional but useful for some evals.

    | Timestamp           | Transaction ID | Amount  | Location      | Device Type | Fraud Label | Target |
    | ------------------- | -------------- | ------- | ------------- | ----------- | ----------- | ------ |
    | 2023-12-01 10:15:23 | TXN001         | 250.00  | New York, USA | Mobile      | 0           | 0      |
    | 2023-12-01 10:17:45 | TXN002         | 5000.00 | London, UK    | Desktop     | 1           | 1      |
    | 2023-12-01 10:20:10 | TXN003         | 1200.00 | Sydney, AUS   | Tablet      | 0           | 0      |
  </Tab>

  <Tab title="Regression">
    To evaluate regression quality, pass a table that contains columns with predicted and actual values. Input features are optional but useful for some evals.

    | Prop ID | Location      | Sq ft | Type      | Bedrooms | Has Garden | Predicted  ($) | Actual ($) |
    | ------- | ------------- | ----- | --------- | -------- | ---------- | -------------- | ---------- |
    | P01     | New York, USA | 850   | Apartment | 2        | No         | 850,000        | 870,000    |
    | P02     | New York, USA | 1200  | House     | 3        | Yes        | 1,250,000      | 1,300,000  |
    | P03     | London, UK    | 950   | Flat      | 2        | No         | 700,000        | 720,000    |
  </Tab>

  <Tab title="Ranking">
    To evaluate ranking or recommendations, pass a table that contains columns with rank/score and interaction results. User/query and item features are optional but useful for some evals.

    | User ID | Movie ID | Movie Title  | Genre         | Average Viewer Rating | Watched Duration (%) | Predicted Rank |
    | ------- | -------- | ------------ | ------------- | --------------------- | -------------------- | -------------- |
    | U001    | M001     | The Matrix   | Sci-Fi        | 4.8                   | 100                  | 1              |
    | U002    | M002     | Titanic      | Romance/Drama | 4.5                   | 80                   | 2              |
    | U001    | M003     | Interstellar | Sci-Fi        | 4.7                   | 90                   | 2              |
  </Tab>

  <Tab title="Embeddings">
    To evaluate embeddings drift, pass embeddings as numerical columns.

    | User ID | Movie ID | Title        | Genre         | Avg Rating | Watched (%) | Predicted Rank |
    | ------- | -------- | ------------ | ------------- | ---------- | ----------- | -------------- |
    | U001    | M001     | The Matrix   | Sci-Fi        | 4.8        | 100         | 1              |
    | U002    | M002     | Titanic      | Romance/Drama | 4.5        | 80          | 2              |
    | U001    | M003     | Interstellar | Sci-Fi        | 4.7        | 90          | 2              |
  </Tab>
</Tabs>

**Data Definition**. Some evaluations require specific columns or data types present.

For example, to evaluate classification quality, you need both predicted labels (prediction) and actual labels (target) columns. To specify where they are located in your table, you can map the data schema using the Data Definition object.

**Two datasets**. Typically you run evaluations use a single dataset (`current` dataset): this is the data you evaluate. Optionally, you can include a second dataset (`reference` dataset). Both datasets must have identical structures.

![](/images/datasets_input_data_two.png)

When to use two datasets:

* **Side-by-side comparison**. Compare outputs or data quality across two periods or model versions in a single Report.

* **Data drift detection. (Required)**. Detect distribution shifts by comparing datasets, such as this week‚Äôs data to the previous one. You always need two datasets.

* **Simplify test setup**. Automatically generate test conditions (e.g., min-max ranges) from the reference dataset without manual configuration.

**Data sampling**. Running evaluations on large datasets (millions of rows) can take time. This depends on the specific evaluation as well as your infrastructure. Since pandas process data in memory, it‚Äôs often more efficient to use samples.

For example, in data drift detection, you can apply random or stratified sampling and compare samples instead of processing the entire dataset.

## Descriptors

To evaluate text data and LLM outputs, you need `Descriptors`.

A Descriptor is a row-level score or label that assesses a specific quality of a given text. It‚Äôs different from Metrics like accuracy which give a score for an entire dataset.

A simple example of a descriptor is `TextLength`. Descriptors range from deterministic to ML and LLM-based checks. For example, descriptors that use the "LLM judge" approach can label responses as "relevant" or "not relevant" using an evaluation prompt sent to a selected LLM.

Descriptors can also use two texts at once, such as checking `SemanticSimilarity` between two columns.

You can use built-in Evidently descriptors and configurable templates (like LLM judges), or add your custom checks in Python. Each Descriptor returns a result that can be:

* **Numerical**. Any scores like symbol count or sentiment score.

* **Categorical**. Labels or binary ‚Äútrue‚Äù/‚Äúfalse‚Äù results for pattern matches.

* **Text string**. Like explanations generated by LLM.

Evidently adds the computed descriptor values directly to the dataset.![](/images/concepts/overview_descriptors_export.png)

This helps with debugging: for example, you can sort to find the negative responses. You can export the results to a Pandas DataFrame or view on the Evidently Platform

After you get the Descriptors, you can use them to compute Metrics and Tests.

## Reports and Metrics

### What is a Metric?

A `Metric` is a key building block for evaluations at the **dataset** or **column** level. Each Metric computes a single value and has an optional visual representation (or several to choose from).

**Using Metrics with Descriptors.** After computing row-level Descriptors, you can use Metrics to aggregate the results across the dataset. For instance, you can calculate the `ValueMean` for sentiment or similarity scores. Descriptors are treated like any other column.

**Using Metrics directly.** You can also compute Metrics directly on the data you have, like a table with ML model logs. You can use Metrics both to understand your data (like column statistics) and to evaluate different aspects of AI quality (like response accuracy).

Metrics range from simple stats like row number to complex algorithmic and prompt-based evals.

<Card title="Explore evaluations" icon="chart-simple" horizontal="horizontal" href="/metrics">
  There is a separate docs section that shows all Descriptors and Metrics.
</Card>

Some examples:

<Tabs>
  <Tab title="ValueMean">
    Compute the mean value of any given column. \[ADD IMG]

    ![](/images/concepts/overview_small_preset_cat_value_example.png)
  </Tab>

  <Tab title="ValueDrift">
    Compute drift score for any individual column, target or prediction. You can set drift detection methods as a parameter.
  </Tab>

  <Tab title="Accuracy">
    Compute classification accuracy.&#x20;
  </Tab>

  <Tab title="MissingValues">
    Compute the share of missing values in the dataset.
  </Tab>

  <Tab title="Diversity">
    Compute the diversity of generated recommendations.
  </Tab>

  <Tab title="MeanError">
    Compute mean error for regression tasks.
  </Tab>
</Tabs>

### What is a Report?

A **Report** is an object you create to structure and execute evaluations.¬†You choose the Metrics to include one by one, run the Report on your dataset, and get a clear visual summary of the results.&#x20;

* You can combine multiple individual Metrics in a Report.¬†

* You can also pass two datasets for a side-by-side comparison for each Metric.¬†

* You can include both built-in Metrics and custom Metrics in the Report.

\[ADD IMG]

![](/images/concepts/overview_small_preset_cat_value_compare_example.png)

However, in many cases, it makes sense to start with **Presets** to simplify the initial setup.

### Metric Presets

Metric Presets are pre-designed evaluation templates. They make it simple to compute multiple related Metrics at once without lengthy setup.

**Small Presets**. Some Presets are ‚Äúsmall‚Äù: they group a handful of Metrics in a single widget. For example:

<Tabs>
  <Tab title="ValueStats">
    You can track all relevant descriptive value statistics at once. The relevant stats will be chosen based on the column or descriptor type (categorical, numerical, text).

    ![](/images/concepts/overview_small_preset_num_value_example-min.png)

    **Comprehensive Presets**. There are also large, more interactive Presets for specific scenarios that include multiple widgets. For example:

    <Tabs>
      <Tab title="Data Drift">
        Identifies shifts in individual column distributions and consolidates the results.

        ![](/images/concepts/overview_drift_report-min.png)
      </Tab>

      <Tab title="Classification">
        Breaks down classification metrics and includes debugging tools like prediction bias plots.
      </Tab>

      <Tab title="RecSys">
        Measures the performance of ranking or recommendation systems.
      </Tab>

      <Tab title="Data Summary">
        Summarizes all dataset columns, generating statistics and profiles for each.
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="Classification">
    Multiple classification quality metrics like Precision, Recall, Accuracy, ROC AUC, etc. together.
  </Tab>

  <Tab title="Dataset">
    A quick overview of all dataset-level stats.
  </Tab>

  <Tab title="Correlations ">
    Correlations between all columns in the dataset on a heatmap.
  </Tab>

  <Tab title="Regression">
    Multiple regession quality metrics like MAE, ME, RMSE, etc.
  </Tab>
</Tabs>

You can freely combine Presets and individual Metrics in a custom Report next to each other.

Reports are great for analysis and debugging. However, in many cases, you don‚Äôt want to review all the scores individually but rather confirm that nothing is off.

## Test Suites and Tests¬†

### What is a Test?

**Tests** let you validate your results against specific expectations. You create a Test by adding a **condition** parameter to a Metric.¬†Each Test will calculate a given value, check it against the rule, and report a pass/fail result.&#x20;

* You can run multiple Tests in one go.&#x20;

* You can create Tests on the dataset or column level.

* You can formulate custom conditions or use defaults.

A **Test Suite** is a collection of individual Test results. It works as an extension to a Report. Once you configure Tests, your Report will have an **additional tab** that shows a summary of outcomes.&#x20;

You can navigate the results by test status or column name.![](/images/concepts/overview_test_suite_example-min.png)

Each Test results in one of the following statuses:

* **Pass:** The condition was met.

* **Fail:** The condition wasn‚Äôt met.

* **Warning:** The condition wasn‚Äôt met, but the check is marked as non-critical.

* **Error:** Something went wrong with the Test itself, such as an execution error.

You can view extra details to debug. Examples:

<Tabs>
  <Tab title="Text length">
    Test that less than 5% of LLM responses fall outside the approved length.&#x20;
    ![](/images/concepts/overview_descriptor_test_example-min.png)
  </Tab>

  <Tab title="LLM correctness">
    Verify that less than 10% of responses are labeled as "incorrect" by the LLM judge.&#x20;
  </Tab>

  <Tab title="Recall">
    Check if recall stays within ¬±10% of a reference model.
  </Tab>

  <Tab title="Value stability">
    Test that the mean value of a column is within ¬±2 of its standard deviation.¬†
  </Tab>

  <Tab title="Dataset drift">
    Test that no more than 50% of features show drift. (Dataset-level Test)
  </Tab>
</Tabs>

### Test Conditions

Evidently has a powerful API to set up Test conditions.&#x20;

* **Manual setup.** You can add thresholds to Metrics one by one, using simple syntax like **greater than (`gt`)** or **less than (`lt`)**.¬†By picking different Metrics to test against, you can formulate fine-grained conditions like "less than 10% of texts can fall outside 10‚Äì100 character length."

* **Manual setup with reference.** If you have a reference dataset (like a previous data batch), you can set conditions **relative** to it. For example, you can check if the min-max value range stays within ¬±5% of the reference range without setting exact thresholds.

* **Automatic setup.** You can run any Test using built-in defaults. These are either:

  * **Heuristics**. For example, the Test on missing values assumes none should be preset.

  * **Heuristics relative to reference.** Here, conditions adjust to a reference. For instance, the Test on missing values assumes their share should stay within ¬±10% of the reference.

### Test Presets

For even faster setup, there are **Test Presets**. Each Metric Preset has a corresponding Test Preset that you can enable as an add-on.¬†When you do this:

* Evidently adds a predefined set of Tests to your Report.

* These Tests use default conditions, either static or inferred from the reference dataset.

For example:¬†

* **Data Summary**. The Metric Preset gives an overview and stats for all columns. The Test Suite checks for quality issues like missing values, duplicates, etc. across all values.

* **Classification.** The Metric Preset shows quality metrics like precision or recall. The Test Suite verifies these metrics against a baseline, like a dummy baseline calculated by Evidently or previous model performance.

## Building your workflow

You can use Evidently Reports and Test Suites on their own or as part of a monitoring system.¬†

### Independent use

Reports are great for exploratory evals:

* **Ad hoc evals.** Run one-time analyses on your data, models or LLM outputs.

* **Experiments.** Compare models, prompts, or datasets side by side.

* **Debugging.** Investigate data or model issues.

Test Suites are great for automated checks like:

* **Data validation.** Test inputs and outputs in prediction pipelines.¬†

* **CI/CD and regression testing.** Check AI system performance after updates.

* **Safety testing**. Run structured behavioral tests like adversarial testing.¬†

For automation, you can integrate Evidently with tools like Airflow. You can trigger actions based on Test results, such as sending alerts or halting a pipeline.

### As part of platform

You can use **Reports** together with the **Evidently Platform** in production workflows:

* Reports serve as a metric computation layer, running evaluations on your data.

* The Platform lets you store, compare results, and track them on Dashboard with alerts.

\[ADD IMG]

This setup works for both experiments and production monitoring. For example:

* **Experiments.** Log evaluations while experimenting with prompts or model versions. Use the Platform to compare runs and track progress.

* **Regression Tests.** Use Test Suites to validate updates on your golden dataset. Debug failures and maintain a history of results on the Platform.

* **Batch Monitoring.** Integrate Reports into your data pipelines to compute Metrics for data batches. Use the Platform for performance tracking and alerting.

Evidently Cloud also offers managed evaluations to generate Reports directly on the platform.

You can choose:

* Self-host the open-source platform version.¬†

* Sign up for [Evidently Cloud](https://www.evidentlyai.com/register) (Recommended).¬†

[Read more on the platform](/docs/platform/overview).