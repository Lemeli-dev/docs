---
title: "Overview"
description: "Evidently Python library at a glance. "
noindex: "true"
---

Evidently library simplifies quality evaluations for AI systems. It supports both predictive ML models and generative AI systems, from classification to RAG.

You can use Evidently as a standalone open-source tool or as part of the Evidently Platform.

\[ADD IMG]

## Workflow

<Steps>
  <Step title="Pass input data">
    Use text, tabular, or embeddings data. Optionally define a schema.
  </Step>

  <Step title="Configure evaluations">
    Pick from 100+ metrics or add yours. Optionally set pass/fail conditions.
  </Step>

  <Step title="Get results">
    Choose your format:

    * Get a visual Report in an interactive Python environment (Jupyter, Colab).

    * Export as JSON, Python dictionary, pandas DataFrame, or HTML file.

    * Upload to the Evidently Platform to store and track over time.
  </Step>
</Steps>

## Evaluation API

Evidently supports 3 evaluation modes with a declarative API.

* `Descriptors` let you compute row-level scores for individual inputs.

* `Metrics` let you compute and aggregate dataset scores.

* `Tests` let you check metrics for pass/fail outcomes. You can set conditions manually or infer them from a reference dataset, like min-max ranges.

## 100+ metrics

Evidently has 100+ built-in evals. Each evaluation has optional **interactive visuals** to explore results, such as feature or score distributions.

You can also implement custom checks in Python or define your prompts for LLM-as-a-judge.

Here are examples of things you can check:

|                                                                                          |                                                                                                        |
| :--------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: |
|                                  **üî° Text descriptors**                                 |                                           **üìù LLM outputs**                                           |
| Length, sentiment, toxicity, language, special symbols, regular expression matches, etc. | Semantic similarity, retrieval relevance, summarization quality, etc. with model- and LLM-based evals. |
|                                    **üõ¢ Data quality**                                   |                                     **üìä Data distribution drift**                                     |
|  Missing values, duplicates, min-max ranges, new categorical values, correlations, etc.  |           20+ statistical tests and distance metrics to compare shifts in data distribution.           |
|                                   **üéØ Classification**                                  |                                            **üìà Regression**                                           |
|            Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.            |                  MAE, ME, RMSE, error distribution, error normality, error bias, etc.                  |
|                                 **üóÇ Ranking (inc. RAG)**                                |                                         **üõí Recommendations**                                         |
|                              NDCG, MAP, MRR, Hit Rate, etc.                              |                         Serendipity, novelty, diversity, popularity bias, etc.                         |