---
title: 'Data definition'
description: 'How to map the input data.'
noindex: true
---

To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:

* **Column types** (e.g., categorical, numerical, text, embeddings).

* **Column roles** (e.g., prediction, target, LLM output, etc.).

This helps Evidently process the data correctly. Some evaluations require certain columns, and will fail if these are missing.

You can create a `DataDefinition` in Python before generating a Report or map the columns visually in the Evidently platform. **Automated data definition** is also available in some cases.

<Info>
  Check data requirements for specific Metrics in the [Reference table](/metrics/all_metrics).
</Info>

## Basic flow

**Import** the following modules:

```python
from evidently.future.datasets import Dataset
from evidently.future.datasets import DataDefinition
```

**Create a Dataset**. You can pass data with [flexible structure](/docs/library/overview#dataset).

* Prepare it as a pandas.DataFrame.

* Create an Evidently Dataset object using `Dataset.from_pandas`.

* Pass the corresponding `data_definition`. For automated mapping, pass an empty `DataDefinition()`object:

```python
eval_data = Dataset.from_pandas(
    pd.DataFrame(source_df),
    data_definition=DataDefinition()
)
```

**Two datasets.** If you're using two datasets (like current and reference for drift detection), create a Dataset object for each. They must have identical data definition.

**Automated data definition.**  If the data definition is not specified, Evidently tries to map columns:

* Based on type (numerical, categorical).

* By matching column names to known roles (e.g., a column "target" treated as the target).

**Manual data definition.** While automation works in many cases, manual mapping is more accurate. It helps avoid mistakes like misclassifying numerical columns with few unique values as categorical. Some evaluations, like text or embedding drift detection require explicit mapping.

<Info>
  Once you have the **Dataset** ready, you can add [text Descriptors ](/docs/library/descriptors)and/or [get Reports](/docs/library/report).
</Info>

You can explore different mapping options below. Note that you only need to use those relevant to your evals. For example, you don’t need roles like target/prediction to run data quality checks.

## Column types

**Why map column types**. This ensures accurate data processing for:

* Statistical analysis. Descriptive stats adjust based on column type.

* Choice of visualizations. This also prevents unsuitable plots (e.g., text columns treated as categorical may get a raw text distribution histogram).

* Data drift analysis. Drift detection methods vary by column type.

* Automated tests choice. Preset composition is mapped to column types.

<Note>
  If you have **text data**, you can [generate descriptors](/docs/library/descriptors) without explicit mapping. However, mapping is recommended since you may later generate dataset-level Reports, like to get a Data Summary.
</Note>

### Examples

**Tabular or text data**. Example mapping:

```python
definition = DataDefinition(
    text_columns=["Latest_Review"],
    numerical_columns=["Age", "Salary"],
    categorical_columns=["Department"],
    datetime_columns=["Joining_Date"]
    )
    
eval_data = Dataset.from_pandas(
    pd.DataFrame(source_df),
    data_definition=definition
)
```

(TBC). **Embeddings** Example mapping:

```python
embeddings = {'small_subset': embeddings_data.columns[:10]} #TBC
```

<Info>
  If you map column types and exclude certain columns, they’ll be ignored in all evaluations.
</Info>

### Options

Available types and how automated mapping works.

| **Column Type**       | **Description**                                                                                                                                                                                         | **Automated Mapping**                               |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| `numerical_columns`   | <ul><li>Columns with numeric values.</li></ul>                                                                                                                                                          | All columns with numeric types (`np.number`).       |
| `datetime_columns`    | <ul><li>Columns with datetime values.</li><li>Ignored in data drift calculations.</li></ul>                                                                                                             | All columns with DateTime format (`np.datetime64`). |
| `categorical_columns` | <ul><li>Columns with categorical values.</li></ul>                                                                                                                                                      | All non-numeric/non-datetime columns.               |
| `text_columns`        | <ul><li>Text columns.</li><li>Mapping **required** for text data drift detection.</li></ul>                                                                                                             | No automated mapping.                               |
| `embeddings` (TBC)    | <ul><li>Columns containing embeddings.</li><li> Pass a dictionary where keys are embedding names and values are lists of columns.</li><li>Mapping **required** for embedding drift detection.</li></ul> | No automated mapping.                               |

## Column roles

Depending on the use case and type of analysis to run, you may need to map specific column roles.

### General

If you have a timestamp or ID column, it's always a good idea to identify them.

| **Column role** | **Description**                                                                                      | **Automated mapping**          |
| --------------- | ---------------------------------------------------------------------------------------------------- | ------------------------------ |
| `id_column`     | <ul><li>Identifier column.</li><li>Ignored in data drift calculations.</li></ul>                     | Column named "id" (TBC)        |
| `timestamp`     | <ul><li>Timestamp column.</li><li> Ignored in data drift calculations (like any DateTime).</li></ul> | Column named "timestamp" (TBC) |

<Info>
  How is`timestamp` different from `datetime_columns`?

  * **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like "date of last contact."

  * **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.
</Info>

### LLM evals

and treat columns appropriately when computing the Metrics

llm=None,

add here on input, target\_output

descriptors auto-mapped but you can map directly too
\When you generate descriptors they are automatically tagged as descriptors.

numerical\_descriptors=\['target2', 'prediction2'], categorical\_descriptors=\[])

#### Descriptors

### Regression

, regression=None,

If you have a dataset with ML inferences, it's necessary to map where Predictions and Actuals are to compute the regression quality metrics.

show example, defaults

### Classification

classification=\[BinaryClassification()]

classification=\[BinaryClassification(name='default', target='target', prediction\_labels=None, prediction\_probas='prediction', pos\_label=1, labels=None)]

If you have a dataset with ML inferences, it's necessary to map where Predictions and Ground Truth are to compute the quality metrics.

show example, defaults

### Ranking

show example, defaults

additional dataset

***

How to define the data schema for ranking and recommendations.

To evaluate data from recommender systems, you must correctly map the input data schema. You can also pass an optional **additional dataset** with training data.

**Note**: this mapping will also apply to search and retrieval systems. Treat "user\_id" as "query\_id".

To evaluate the quality of a ranking or a recommendation system, you must pass:

* The score or rank generated by the system as the prediction.

* The relevance labels as the target (e.g., this could be an interaction result like user click, assigned relevance label, etc.)

Here are the examples of the expected data inputs.

If the model prediction is a score (expected by default):

| user\_id | item\_id | prediction (score) | target (relevance) |
| -------- | -------- | ------------------ | ------------------ |
| user\_1  | item\_1  | 1.95               | 0                  |
| user\_1  | item\_2  | 0.8                | 1                  |
| user\_1  | item\_3  | 0.05               | 0                  |

If the model prediction is a rank:

| user\_id | item\_id | prediction (rank) | target (relevance) |
| -------- | -------- | ----------------- | ------------------ |
| user\_1  | item\_1  | 1                 | 0                  |
| user\_1  | item\_2  | 2                 | 1                  |
| user\_1  | item\_3  | 3                 | 0                  |

The **target** column with the interaction result or relevance label can contain either:

* a binary label (where `1` is a positive outcome)

* any true labels or scores (any positive values, where a higher value corresponds to a better match or a more valuable user action).

You might need to add additional details about your dataset via column mapping:

* `recommendations_type`: `score` (default) or `rank`. Helps specify whether the prediction column contains ranking or predicted score.

* `user_id`: helps specify the column that contains user IDs.

* `item_id`: helps specify the column that contains ranked items.