---
title: 'Overview'
description: 'End-to-end evaluation workflow.'
noindex: true
---

This page shows the core evaluation workflow with the open-source Evidently library at glance and links to guides on specific components. You can adapt this workflow for all evaluation scenarios (experiments, regression testing, etc.) and use cases: from data to LLM.

<Tip>
  To log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It's optional: you can also run evals locally.
</Tip>

## Define and run the eval

<Steps>
  <Step title="Prepare the input data" icon="asterisk">
    Get your data in a tabular format like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#input_data). You can also download data from the Evidently Platform (like [pull tracing a dataset](/docs/platform/datasets_workflow)).
  </Step>

  <Step title="Create a Dataset object" icon="asterisk">
    You must create a Dataset object and set the `DataDefinition()`. This lets you specify column role / types. Data Definition is necessary for some evals. If empty, Evidently will try guessing the column types. [How to set Data Definition](/docs/library/data_definition).

    ```python
    eval_data = Dataset.from_pandas(
        pd.DataFrame(source_df),
        data_definition=DataDefinition()
    )
    ```
  </Step>

  <Step title="(Optional) Add descriptors" icon="plus">
    If you run evals on text data (text length, sentiment, LLM judges), add row-level `descriptors` to the Dataset.  [How to use Descriptors](/docs/library/descriptors).

    ```python
    eval_data.add_descriptors(descriptors=[
        TextLength("Question", alias="Length"),
        Sentiment("Answer", alias="Sentiment")
    ])
    ```
  </Step>

  <Step title="Configure Report" icon="asterisk">
    To set up dataset-level evals (classification, data drift) and to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).

    ```python
    report = Report([
        DataSummaryPreset()
    ])
    ```
  </Step>

  <Step title="(Optional) Add Test conditions" icon="plus">
    You can set Pass/Fail conditions, like to check if text length in \< 100 symbols. How to [configure Tests](/docs/library/tests).

    ```python
    report = Report([
        DataSummaryPreset(),
        MaxValue(column="Length", tests=[lt(100)]),
    ])
    ```
  </Step>

  <Step title="(Optional). Add Tags and Timestamps" icon="plus">
    You can add `tags` or `metadata` to identify specific evaluation runs or datasets, or override a `timestamp ` to associate an eval with a specific period. [How to add metadata](/docs/library/tags_metadata).
  </Step>

  <Step title="Run the Report" icon="asterisk">
    Finally, you can `run`the Report on the `Dataset` (or two) to execute the eval.&#x20;

    ```python
    my_eval = report.run(eval_data_1, None)
    ```
  </Step>

  <Step title="Explore the results" icon="asterisk">
    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).

    ```python
    ws.add_report(project.id, my_eval, include_data=True)
    ```

    * To view locally. [All output formats](/docs/library/output_formats).

    ```python
    my_eval
    ##my_eval.json()
    ```
  </Step>
</Steps>