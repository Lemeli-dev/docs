---
title: 'Overview'
description: 'General evaluation workflow.'
noindex: true
---

## Connect to workspace

If you will be logging your evals to an Evidently Workspace (cloud or self-hosted), you must first connect to a workspace and create a new or connect to an existing project.&#x20;

```python
ws.get_project("PROJECT_ID")
```

## Define and run the eval

<Steps>
  <Step title="Data Definition">
    You run each evaluation on a Dataset.&#x20;

    * Prepare your data as a Pandas DataFrame. You can prep your input data locally or load it from the Evidently Platform (for example, download a tracing dataset: check [Dataset API](/docs/platform/datasets_workflow).

    * Create a Dataset object.

    * Define the feature types is optional but mostly recommended.

    * Defining data schema is necessary for some evals.
  </Step>

  <Step title="(Optonal). Select Descriptors">
    If you run evaluation on text data, choose your descriptors.&#x20;
  </Step>

  <Step title="Configure Report">
    * Create a `Report` object with the chosen `metrics`  or `metric_presets`.

    * Optionally, pass custom parameters to a `Metric` to customize for Metric calculations.
  </Step>

  <Step title="(Optional). Add Test conditions">
    Optionally, pass custom parameters to a `Metric` to add `Tests` to run conditional Pass/Fail checks.
  </Step>

  <Step title="(Optional). Add Tags ">
    * Optional: add a `tags` or `metadata` to identify this specific evaluation run.
  </Step>

  <Step title="(Optional). Add Timestamp">
    * Optional: and a custom `timestamp` to the current run.
  </Step>

  <Step title="Run the Report">
    * Execute the evaluation on your dataset.

    * Pass the `current` dataset you want to evaluate or profile. Optional (required for data distribution checks): pass the `reference` dataset.
  </Step>
</Steps>

## Get the results

After you compute the Report or Test Suite, use the `add_report` or `add_test_suite` methods to send them to a corresponding Project in your workspace.

## Or Get the results&#x20;

Choose how to get the outputs.&#x20;

* **View the Report** in an interactive Python environment (Jupyter, Colab).

* **Export** as JSON, Python dictionary, pandas DataFrame, or HTML file.

* **Upload** to the Evidently Platform to store and track over time.

##

## Delete snapshots

To delete snapshots in the Workspace `ws`, pass the Project ID and snapshot ID. You can see the snapshot ID on the Report or Test Suite page.

Copy

`ws.delete_snapshot(project_id, snapshot_id)`

[](https://docs.evidentlyai.com/user-guide/evaluations/snapshots#examples)
Examples
--------

### [](https://docs.evidentlyai.com/user-guide/evaluations/snapshots#send-snapshots)&#xA;Send snapshots

**Report**. To create and send a Report with data summaries for a single dataset `batch1` to the workspace `ws`:

Copy

`data_report = Report(`
`      metrics=[`
`          DataQualityPreset(),`
`      ],`
`   )`
`data_report.run(reference_data=None, current_data=batch1)`
`ws.add_report(project.id, data_report)`

**Test Suite**. To create and send a Test Suite with data drift checks, passing current and reference data:

Copy

`drift_checks = TestSuite(tests=[`
`  DataDriftTestPreset(),`
`])`
`drift_checks.run(reference_data=reference_batch, current_data=batch1)`
`ws.add_test_suite(project.id, drift_checks)`

**Send a snapshot**. The `add_report` or `add_test_suite` methods generate snapshots automatically. If you already have a snapshot (e.g., you previously saved it), you can load it load and send it to your Project:

Copy

`ws.add_snapshot(project.id, snapshot.load("data_drift_snapshot.json"))`

**Snapshot size**. A single upload to Evidently Cloud should not exceed 50MB (Free plan) or 500MB (Pro plan). This limitation applies to the size of the JSON, not the dataset itself. Example: a data drift report for 50 columns and 10,000 rows of current and reference data results in a snapshot of approximately 1MB. (For 100 columns x 10,000 rows: \~ 3.5MB; for 100 columns x 100,000 rows: \~ 9MB). The size varies depending on the metrics or tests used.

### [](https://docs.evidentlyai.com/user-guide/evaluations/snapshots#add-dataset)&#xA;Add dataset

When you upload a Report or Test Suite, you can optionally include the Dataset you evaluated, together with added Descriptors (if any). This helps with row-level debugging and analysis.

Use the `include_data` parameters (defaults to False):

Copy

`ws.add_report(project.id, data_report, include_data=True)`