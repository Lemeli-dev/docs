---
title: "Custom LLM Judge"
description: "How to run prompt-based evaluators for custom criteria."
noindex: "true"
---

**Pre-requisites**:

* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data.

LLM-based descriptors use an external LLM for evaluation. You can use built-in descriptors (with pre-written evaluation prompts) or customize one of our templates for your specific criteria.

## Imports

```python
import pandas as pd

from evidently.future.datasets import Descriptor
from evidently.features.llm_judge import BinaryClassificationPromptTemplate
from evidently.future.descriptors import LLMEval, ToxicityLLMEval, ContextQualityLLMEval, DeclineLLMEval
```

<Accordion title="Toy data to run the example" defaultOpen={false}>
  To generate toy data and create a Dataset object:

  ```python
  from evidently.future.datasets import Dataset
  from evidently.future.datasets import DataDefinition

  data = [
      ["Why is the sky blue?", 
       "The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.", 
       "because air scatters blue light more"],
      ["How do airplanes stay in the air?", 
       "Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.", 
       "because wings create lift"],
      ["Why do we have seasons?", 
       "We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.", 
       "because Earth is tilted"],
      ["How do magnets work?", 
       "Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.", 
       "because of magnetic fields"],
      ["Why does the moon change shape?", 
       "The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.", 
       "because it rotates"],
      ["What movie should I watch tonight?", 
       "A movie is a motion picture created to entertain, educate, or inform viewers through a combination of storytelling, visuals, and sound.", 
       "watch a movie that suits your mood"]
  ]

  columns = ["question", "context", "response"]

  df = pd.DataFrame(data, columns=columns)

  ```
</Accordion>

## Built-in LLM evals

<Tip>
  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#llm-based-evals).
</Tip>

You can choose from built-in LLM-based descriptors for popular criteria, like detecting if the text contains a refusal to answer. Each LLM-based descriptor will return a label (default), the reasoning for the decision (default), and an optional score.

(TBC) Built-in descriptors default to using `gpt-4o-mini` model from OpenAI.

**OpenAI key.** Add the token as the environment variable: [see docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).&#x20;

```python
import os
os.environ["OPENAI_API_KEY"] 
```

**Run a single-column eval.** For example, to evaluate whether `response`contains any toxicity:

```python
eval_df.add_descriptors(descriptors=[
    ToxicityLLMEval("response", alias="toxicity"),
])
```

View the results as usual:

```
eval_df.as_dataframe()
```

**Run a multi-column eval.** Some descriptors like response relevance or completeness naturally require two columns. For example, to evaluate Context Quality (whether it contains enough information to answer the question), you must run this evaluation over your `context` column, and pass the `question` column as a parameter.

```python
eval_df.add_descriptors(descriptors=[
    ContextQualityLLMEval("context", alias="good_context", question="question"),
])
```

(TBC) **Parametrize evaluators**. You can switch the output format from `category` to `score` (0 to 1) or exclude the reasoning:

```python
eval_df.add_descriptors(descriptors=[
    DeclineLLMEval("response", alias="refusal", include_reasoning=False),
    ToxicityLLMEval("response", alias="toxicity", include_category=False),
    PIILLMEval("response", alias="PII", include_score=True), 
])
```

**Change the LLM**. You can choose to use a different model for the evals.

```python
eval_df.add_descriptors(descriptors=[
    ToxicityLLMEval("response", alias="toxicity", provider="openai", model="gpt-3.5-turbo"),
])
```

## Custom LLM evals

You can also create a custom LLM evaluator using the provided **templates**. You specify the parameters and evaluation criteria, and Evidently will generate the complete evaluation prompt to send to the LLM together with the evaluation data.

**Binary classification template**. Include the definition of your `criteria`, names of categories, etc. For example, to define the prompt for "conciseness" evaluation:

```python
conciseness = BinaryClassificationPromptTemplate(
        criteria = """Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.
            A concise response should:
            - Provide the necessary information without extra details or repetition.
            - Be brief yet comprehensive enough to address the query.
            - Use simple and direct language to convey the message effectively.
        """,
        target_category="concise",
        non_target_category="verbose",
        uncertainty="unknown",
        include_reasoning=True,
        pre_messages=[("system", "You are a judge which evaluates text.")],
        )      
```

You do not need to explicitly ask the LLM to classify your input into two classes, ask for reasoning, or format it specially. This is already part of the Evidently template.

To apply this descriptor for your data, pass the template name to the `LLMEval` descriptor:

```python
eval_df.add_descriptors(descriptors=[
    LLMEval("response", template=conciseness, provider = "openai", model = "gpt-4o-mini", alias="Conciseness"),
])
```

Publish results as usual:

```python
eval_df.as_dataframe()
```

This template is very flexible. For example, you can use to decide if the question is appropriate to the scope of your LLM application. A simplified example:

```python
appropriate_scope = BinaryClassificationPromptTemplate(
        pre_messages=[("system", "You are a judge which evaluates questions sent to a student tutoring app.")],
        criteria = """An appropriate question is any educational query related to
        - academic subjects (e.g., math, science, history)
        - general world knowledge or skills
        An inappropriate question is any question that is:
        - unrelated to educational goals, such as personal preferences, pranks, or opinions
        - offensive or aimed to provoke a biased response.
        """,
        target_category="appropriate",
        non_target_category="inappropriate",
        uncertainty="unknown",
        include_reasoning=True,
        )
```

(TBC) **Using two columns**. To create a custom evaluator that uses more than one column (similar to the Context Quality example above), pass the second column in the list of `additional_columns` and then reference this`{column_name}`the inside your evaluation criteria. This is useful for scenarios like classifying responses based on their faithfulness to the context, etc.

```python
ADD EXAMPLE
```

See the explanation of each parameter below.

You do not need to explicitly include the name of your primary column in the evaluation prompt. Evidently will conside the column that you compute the descriptor and it will be automatically passed to the template.

```
ADD EXAMPLE
```

## (TBC) Parameters

### LLMEval Parameters

| Parameter  | Description                                        | Options                                                            |
| ---------- | -------------------------------------------------- | ------------------------------------------------------------------ |
| `template` | Sets a specific template for evaluation.           | `BinaryClassificationPromptTemplate`                               |
| `provider` | The provider of the LLM to be used for evaluation. | `openai`                                                           |
| `model`    | Specifies the model used for evaluation            | Any available provider model (e.g., \`gpt-3.5-turbo\`, \`gpt-4\`)  |

### BinaryClassificationPromptTemplate

| Parameter             | Description                                                                                                                                                                                                                                                                                                  | Options                                                     |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------- |
| `criteria`            | Free-form text defining evaluation criteria.                                                                                                                                                                                                                                                                 | Custom string (required)                                    |
| `target_category`     | Name of the target category you want to detect (e.g., you care about its precision/recall more than the other). 
<br /> <br />
Often there is no difference which category you call "target": there has no impact to evaluation. But this can be useful if you later run quality evaluations for your LLM judge. | Custom category (required)                                  |
| `non_target_category` | Name of the non-target category.                                                                                                                                                                                                                                                                             | Custom category (required)                                  |
| `uncertainty`         | Category to return when the provided information is not sufficient to make a clear determination.                                                                                                                                                                                                            | `unknown` (Default), `target`, `non_target`                 |
| `include_reasoning`   | Specifies whether to include the LLM-generated explanation of the result.                                                                                                                                                                                                         | `True` (Default), `False`                                   |
| `pre_messages`        | List of system messages that set context or instructions before the evaluation task. For example, you can explain the evaluator role ("you are an expert..") or context ("your goal is to grade the work of an intern..").                                                                                   | Custom string (optional)                                    |
| `additional_columns`  | A dictionary of additional columns present in your dataset to include in the evaluation prompt. Use it to map the column name to the placeholder name you reference in the `criteria`. For example: `({"mycol": "question"}`.                                                                                | Custom dictionary (optional)                                |

<Accordion title="OpenAIPrompting descriptor" defaultOpen={false}>
  # OpenAIPrompting

  There is an earlier implementation of this approach with `OpenAIPrompting` descriptor. See the documentation below.

  OpenAIPrompting Descriptor

  To import the Descriptor:

  ```python
  from evidently.descriptors import OpenAIPrompting
  ```

  Define a prompt. This is a simplified example:

  ```python
  pii_prompt = """
  Please identify whether the below text contains personally identifiable information, such as name, address, date of birth, or other.
  Text: REPLACE 
  Use the following categories for PII identification:
  1 if text contains PII
  0 if text does not contain PII
  0 if the provided data is not sufficient to make a clear determination
  Return only one category.
  """
  ```

  The prompt has a REPLACE placeholder that will be filled with the texts you want to evaluate. Evidently will take the content of each row in the selected column, insert into the placeholder position in a prompt and pass it to the LLM for scoring.

  To compute the score for the column `response` and get a summary Report:

  ```python
  openai_prompting = Dataset.from_pandas(
      pd.DataFrame(data),
      data_definition=data_definition,
      descriptors=[
          OpenAI("Answer", prompt=pii_prompt, prompt_replace_string="REPLACE", model="gpt-3.5-turbo-instruct", 
                 feature_type="num", alias="PII for Answer (by gpt3.5)"),
          
      ]
  )
  ```

  View as usual:

  ```
  openai_prompting.as_dataframe()
  ```

  ## Descriptor parameters

  * **`prompt: str`**

    * The text of the evaluation prompt that will be sent to the LLM.

    * Include at least one placeholder string.

  * **`prompt_replace_string: str`**

    * A placeholder string within the prompt that will be replaced by the evaluated text.

    * The default string name is "REPLACE".

  * **`feature_type: str`**

    * The type of Descriptor the prompt will return.

    * Available types: `num` (numerical) or `cat` (categorical).

    * This affects the statistics and default visualizations.

  * **`context_replace_string: str`**

    * An optional placeholder string within the prompt that will be replaced by the additional context.

    * The default string name is "CONTEXT".

  * **`context: Optional[str]`**

    * Additional context that will be added to the evaluation prompt, which **does not change** between evaluations.

    * Examples: a reference document, a set of positive and negative examples, etc.

    * Pass this context as a string.

    * You cannot use `context` and `context_column` simultaneously.

  * **`context_column: Optional[str]`**

    * Additional context that will be added to the evaluation prompt, which is **specific to each row**.

    * Examples: a chunk of text retrieved from reference documents for a specific query.

    * Point to the column that contains the context.

    * You cannot use `context` and `context_column` simultaneously.

  * **`model: str`**

    * The name of the OpenAI model to be used for the LLM prompting, e.g., `gpt-3.5-turbo-instruct`.

  * **`openai_params: Optional[dict]`**

    * A dictionary with additional parameters for the OpenAI API call.

    * Examples: temperature, max tokens, etc.

    * Use parameters that OpenAI API accepts for a specific model.

  * **`possible_values: Optional[List[str]]`**

    * A list of possible values that the LLM can return.

    * This helps validate the output from the LLM and ensure it matches the expected categories.

    * If the validation does not pass, you will get `None` as a response label.

  * **`alias: Optional[str]`**

    * A display name visible in Reports and as a column name in tabular export.

    * Use it to name your Descriptor.
</Accordion>