---
title: "Overview"
description: "Available evaluations and how to customize them."
noindex: "true"
mode: "wide"
---

Evaluations are a core feature of the Evidently library. There 100+ evals with built-in visuals. Before exploring, make sure you're familiar with the eval workflow (for [LLMs](../quickstart_llm), for [ML](../quickstart_ml)) and these key concepts:

* **Descriptors** return row-level scores (like text length) or labels (like “PII present”) for each input.
* **Metrics** work at the dataset or column level. These are checks like accuracy or  data drift. You also need Metrics to summarize row-level results.
* **Tests** let you add optional Pass/Fail checks on top of Metrics.
* **Reports** are how you group and visualize the evaluation results.
* **Presets** are pre-configured Reports for common scenarios.

## Popular links

<CardGroup cols={4}>
  <Card title="All Descriptors" icon="table-list" href="all_descriptors">
    Reference table.
  </Card>

  <Card title="All Metrics" icon="chart-simple" href="all_metrics">
    Reference table.
  </Card>

  <Card title="LLM judges" icon="sparkles" href="customize_llm_judge">
    How to create.
  </Card>

  <Card title="Data Drift" icon="sparkles" href="explainer_data_drift">
    How to detect.
  </Card>
</CardGroup>