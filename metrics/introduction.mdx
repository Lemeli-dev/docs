---
title: "Available evaluations"
description: "Metrics and Presets."
noindex: "true"
---

This section of the docs explains how evaluations work in Evidently, and lists all available Metrics (built-in evals) and Presets (templated combinations of Metrics).

This is a reference page. It shows all the available Metrics, Descriptors and Presets.

You can use the menu on the right to navigate the sections. We organize the Metrics by logical groups. Note that these groups do **not** match the Presets with a similar name. For example, there are more Data Quality Metrics than included in the `DataQualityPreset`.

# How to read the tables

* **Name**: the name of the Metric.

* **Description**: plain text explanation. For Metrics, we also specify whether it applies to the whole dataset or individual columns.

* **Parameters**: required and optional parameters for the Metric or Preset. We also specify the defaults that apply if you do not pass a custom parameter.

**Metric visualizations**. Each Metric includes a default render. To see the visualization, navigate to the examples and run the notebook with all Metrics or Metric Presets.

We are doing our best to maintain this page up to date. In case of discrepancies, check the "All metrics" notebook in examples. If you notice an error, please send us a pull request with an update!

# Metric Presets

**Defaults**: Presets use the default parameters for each Metric. You can see them in the tables below.

### Data Quality Preset

`DataQualityPreset` captures column and dataset summaries. Input columns are required. Prediction and target are optional.

**Composition**:

* `DatasetSummaryMetric()`

* `ColumnSummaryMetric()` for `all` or specified `columns`

* `DatasetMissingValuesMetric()`

**Optional parameters**:

* `columns`

### Data Drift Preset

`DataDriftPreset` evaluates the data distribution drift in all individual columns, and share of drifting columns in the dataset. Input columns are required.

**Composition**:

* `DataDriftTable()` for all or specified `columns`

* `DatasetDriftMetric()` for all or specified `columns`

**Optional parameters**:

* `columns`

* `stattest`

* `cat_stattest`

* `num_stattest`

* `per_column_stattest`

* `text_stattest`

* `stattest_threshold`

* `cat_stattest_threshold`

* `num_stattest_threshold`

* `per_column_stattest_threshold`

* `text_stattest_threshold`

* `embeddings`

* `embeddings_drift_method`

* `drift_share`

How to set data drift parameters, embeddings drift parameters.

### Target Drift Preset

`TargetDriftPreset` evaluates the prediction or target drift. Target and/or prediction is required. Input features are optional.

**Composition**:

* `ColumnDriftMetric()` for `target` and/or `prediction` columns

* `ColumnCorrelationsMetric()`  for `target` and/or `prediction` columns

* `TargetByFeaturesTable()` for all or specified `columns`

* `ColumnValuePlot()` for `target` and/or `prediction` columns - if the task is `regression`

**Optional parameters**:

* `columns`

* `stattest`

* `cat_stattest`

* `num_stattest`

* `per_column_stattest`

* `stattest_threshold`

* `cat_stattest_threshold`

* `num_stattest_threshold`

* `per_column_stattest_threshold`

How to set data drift parameters.

### Regression Preset

`RegressionPreset` evaluates the quality of a regression model. Prediction and target are required. Input features are optional.

**Composition**:

* `RegressionQualityMetric()`

* `RegressionPredictedVsActualScatter()`

* `RegressionPredictedVsActualPlot()`

* `RegressionErrorPlot()`

* `RegressionAbsPercentageErrorPlot()`

* `RegressionErrorDistribution()`

* `RegressionErrorNormality()`

* `RegressionTopErrorMetric()`

* `RegressionErrorBiasTable()` for all or specified `columns`

**Optional parameters**:

* `columns`

### Classification Preset

`ClassificationPreset` evaluates the quality of a classification model. Prediction and target are required. Input features are optional.

**Composition**:

* `ClassificationQualityMetric()`

* `ClassificationClassBalance()`

* `ClassificationConfusionMatrix()`

* `ClassificationQualityByClass()`

* `ClassificationClassSeparationPlot()` - if probabilistic classification

* `ClassificationProbDistribution()` - if probabilistic classification

* `ClassificationRocCurve()` - if probabilistic classification

* `ClassificationPRCurve()` - if probabilistic classification

* `ClassificationPRTable()` - if probabilistic classification

* `ClassificationQualityByFeatureTable()` for all or specified `columns`

**Optional parameters**:

* `columns`

* `probas_threshold`

### Text Evals

`TextEvals()` provides a simplified interface to list `Descriptors` for a given text column. It returns a summary of evaluation results.

**Composition**:

* `ColumnSummaryMetric()` for text descriptors for the specified text column:

  * `Sentiment()`

  * `SentenceCount()`

  * `OOV()`

  * `TextLength()`

  * `NonLetterCharacterPercentage()`

**Required parameters**:

* `column_name`

**Optional parameters**:

* `descriptors` list

### RecSys (Recommender System) Preset

`RecsysPreset` evaluates the quality of the recommender system. Recommendations and true relevance scores are required. For some metrics, training data and item features are required.

**Composition**:

* `PrecisionTopKMetric()`

* `RecallTopKMetric()`

* `FBetaTopKMetric()`

* `MAPKMetric()`

* `NDCGKMetric()`

* `MRRKMetric()`

* `HitRateKMetric()`

* `PersonalizationMetric()`

* `PopularityBias()`

* `RecCasesTable()`

* `ScoreDistribution()`

* `DiversityMetric()`

* `SerendipityMetric()`

* `NoveltyMetric()`

* `ItemBiasMetric()` (pass column as a parameter)

* `UserBiasMetric()` (pass column as a parameter)

**Required parameter**:

* `k`

**Optional parameters**:

* `min_rel_score: Optional[int]`

* `no_feedback_users: bool`

* `normalize_arp: bool`

* `user_ids: Optional[List[Union[int, str]]]`

* `display_features: Optional[List[str]]`

* `item_features: Optional[List[str]]`

* `user_bias_columns: Optional[List[str]]`

* `item_bias_columns: Optional[List[str]]`

| Metric              | Desc                                     | Parameters                                            | Condition                                    |
| ------------------- | ---------------------------------------- | ----------------------------------------------------- | :------------------------------------------- |
| **RecallTopK()**    | Calculates the recall at `k`. Works fast | **Required**: <ul><li>Item 1</li><li>Item 2</li></ul> | Auto test condition: +/- 10% from reference. |
| **PrecisionTopK()** | Calculates the precision at `k`.         | **Required**:                                         |                                              |