---
title: "Overview"
description: "Available evaluations and how to customize them."
noindex: "true"
mode: "wide"
---

Built-in evaluations as a core Evidently feature. There are 100+ evals with visualizations and relevant parameters.

Before exploring, try runnning a single eval (for [LLMs](../quickstart_llm), for [ML](../quickstart_ml)) and learn the key components:
* **Descriptors** evaluate text data row-by-row, returning scores like a "hallucination" label or text length for each input.
* **Metrics** work at the dataset or column level. These are checks like accuracy, data drift, or column means. You also need Metrics to summarize row-level results.
* **Tests** let you add optional Pass/Fail checks on top of Metrics.
* **Reports** are how you group and visualize the evaluation results.

# Popular links

<CardGroup cols={4}>
  <Card title="All Descriptors" icon="table-list" href="all_descriptors">
    Reference table.
  </Card>

  <Card title="All Metrics" icon="chart-simple" href="all_metrics">
    Reference table.
  </Card>

  <Card title="LLM judges" icon="sparkles" href="customize_llm_judge">
    How to create.
  </Card>

  <Card title="Data Drift" icon="sparkles" href="explainer_data_drift">
    How to detect.
  </Card>
</CardGroup>
