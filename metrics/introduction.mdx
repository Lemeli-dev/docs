---
title: "Overview"
description: "How to navigate available evaluations."
noindex: "true"
---

Built-in evaluations as a core feature of Evidently. This section covers what is available and how to customize them.

Before exploring metrics, make sure you know how to run a single eval ([LLM Quickstart](../quickstart_llm), [ML Quickstart](../quickstart_ml) and the key components:

* **Descriptors** evaluate text data row-by-row. They return scores for each input, like a "hallucination" label or text length.

* **Metrics** work at the dataset or column level. They give scores for the whole dataset, like classification accuracy, data drift, or data stats like column means. You also need Metrics to summarize row-level results.

* **Tests** let you add optional Pass/Fail checks on top of Metrics.

* **Reports** are how you group and visualize the evaluation results.

# Popular links

<CardGroup cols={4}>
  <Card title="All Descriptors" icon="table-list" href="all_descriptors">
    Reference table.
  </Card>

  <Card title="All Metrics" icon="chart-simple" href="all_metrics">
    Reference table.
  </Card>

  <Card title="LLM judges" icon="sparkles" href="customize_llm_judge">
    How to create.
  </Card>

  <Card title="Data Drift" icon="sparkles" href="explainer_data_drift">
    How to detect.
  </Card>
</CardGroup>