---
title: "Overview"
description: "Available evaluations and how to customize them."
noindex: "true"
mode: "wide"
---

Evaluations are a core feature of the Evidently library. It offers both a catalog of 100+ ready-made evaluations, and a framework to easily implement and configure yours. You can use Evidently to test and evaluate:
* **LLM-based applications**: summarization, Q\&A, chatbots, agents, RAGs, etc.
* **Predictive ML models**: classification, regression, ranking, recommendations.
* **Data quality** and **data drift** for text, tabular data, embeddings.

Before exploring, make sure you are familiar with the core evaluation workflow: try an example for [LLMs](../quickstart_llm) or [ML](../quickstart_ml).

<CardGroup cols={3}>
  <Card title="All Descriptors" icon="table-list" href="all_descriptors">
    Reference table for single-input text evals.
  </Card>

  <Card title="All Metrics" icon="chart-simple" href="all_metrics">
    Reference table for dataset evals and test default.
  </Card>

  <Card title="All Presets" icon="files" href="all_presets">
    Navigation page for pre-built eval templates.
  </Card>
</CardGroup>

## Popular links

<CardGroup cols={2}>
  <Card title="LLM judges" icon="sparkles" href="customize_llm_judge">
    How to create a custom LLM judge.
  </Card>

  <Card title="Data Drift" icon="chart-waterfall" href="customize_data_drift">
    How to customize data drift detection.
  </Card>
</CardGroup>