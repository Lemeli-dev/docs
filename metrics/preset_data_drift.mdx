---
title: 'Data Drift'
description: 'Overview of the Data Drift Preset.'
noindex: "true"
---

To run a Preset on your data:

```python
report = Report(metrics=[
    DataDriftPreset(),
])
```

To add Tests:

\[ADD CODE]

## Overview

`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.

![](/images/metrics/preset_data_drift_2-min.png)

* **Column drift.** Checks for shifts in each column. The drift detection method is chosen automatically based on the column type and number of observations.

* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.

* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.

<Info>
  **Data Drift Explainer.** Check the [Data Drift Metrics](/metrics/explainer_drift) to understand how different methods work.
</Info>

The table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.

![](/images/metrics/preset_data_drift-min.png)

If you choose Tests, you will get an alternative Test Suite view.

\[ADD IMG]

## Use cases

You can evaluate data drift in different scenarios.

* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor the feature drift to check if the model still operates in a familiar environment. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.

* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.

* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.

* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.

<Info>
  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).
</Info>

## Data requirements

* **Input columns.** You can provide any input columns. They must be non-empty.

* **Column types.** The Preset can evaluate drift for numerical, categorical or text data. You can explicitly specify each column type. Otherwise, Evidently will try to identify the numerical and categorical features automatically. It is recommended to use data definition to avoid errors. If you have text data, you must always map it.

<Info>
  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.
</Info>

## Report customization

**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.

**Change drift parameters.** Evidently has a large number of drift detection methods (univariate and multivariate), including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can specify them or change the overall drift detection conditions. You can also pick tests by column or implement a custom drift method.

<Info>
  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/docs/metrics/customize_data_drift).
</Info>

**Modify Report**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.

* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift("prediction"` to your Report so that you see the drift in this value in a separate widget.

* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive statistics and run data quality Tests, like detecting missing values. Note that the data drift checks drop the null values (and compare the distributions of non-empty features), so you may want to run such Tests separately.

* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.

* **Detect embedding drift**. To detect drift in embeddings data, use `EmbeddingDrift`.

<Info>
  **Creating a custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.
</Info>

**\[ADD on feature importances]**