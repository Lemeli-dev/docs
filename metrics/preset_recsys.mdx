---
title: "RecSys Preset"
description: "Overview of the Recommender Systems Preset"
noindex: "true"
---

To run a Preset on your data:

```python
report = Report(metrics=[
    RecsysPreset(k=5),
])
column_mapping = ColumnMapping(recommendations_type='rank', target='rating', prediction='rank', item_id='title', user_id='user_id')
report.run(
    reference_data=reference,
    current_data=current,
    column_mapping=column_mapping,
    additional_data={'current_train_data': train_data}
  )
report
```

To add Tests

\[ADD CODE]

## Overview

![](/images/metrics/preset_recsys-min.png)

**Scope.**`RecsysPreset` evaluates the quality of the recommender system by generating multiple metrics to assess the quality of ranking and diversity of recommended content. 

**Use case**. When to use the Preset:

* **Experimental evaluations** as you iterate on building your recommender system.

* **Side-by-side comparison** for two different models or periods.&#x20;

* **Production monitoring** checks after you acquire ground truth labels.&#x20;

**Composition**. 10+ metrics like NDCG at K, MAP at K, HitRate, diversity, serendipity, etc. You must provide a `k` parameter to evaluate the Top-K recommendations.

<Info>
  Check the complete list of Metrics included in each Preset on the [All Metrics page](/docs/metrics/all-metrics).
</Info>

<Info>
  Check the Ranking [Metrics Guide](/metrics/explainer_recsys) for an explanation of each metric.
</Info>

**Data requirements.** You must provide the following inputs:

* **Prediction:** recommended items with rank or score

* **Target:** True relevance score

* (Optional) **Input/user features**: for some diversity metrics

* (Optional) **Training data**: for some diversity metrics

* (Optional) **Reference dataset**. Provide to get a side-by-side comparison.

<Info>
  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.
</Info>

## Report customization

**Test conditions.**  If you enable Tests and provide a reference dataset, Evidently will use the default Test conditions dynamically learned from the reference. If you want to set your own conditions, you must create a custom Report and add conditions to the indiviudal Metrics.

<Info>
  **Adding Test conditions**. Check how to specify [Tests](/docs/library/tests) for your Report.
</Info>

**Metric composition**. If you want to add or remove some of the Metrics, you must create a custom Report listing the individual Metrics. This is straightforward. You can also implement fully custom Metrics.

<Info>
  **Creating a custom Report**. Check how to create a [Report](/docs/library/report).
</Info>