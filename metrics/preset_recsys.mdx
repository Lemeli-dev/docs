---
title: "Recommendations"
description: "Overview of the Recommender Systems Preset"
noindex: "true"
---

To run a Preset on your data:

```python
report = Report(metrics=[
    RecsysPreset(k=5),
])
column_mapping = ColumnMapping(recommendations_type='rank', target='rating', prediction='rank', item_id='title', user_id='user_id')
report.run(
    reference_data=reference,
    current_data=current,
    column_mapping=column_mapping,
    additional_data={'current_train_data': train_data}
  )
report
```

To add Tests

\[ADD CODE]

## Overview

`RecsysPreset` evaluates the quality of the recommender system by generating multiple metrics to assess the quality of ranking and diversity of recommendations. You must provide the `k` parameter to evaluate the Top-K recommendations.

![](/images/metrics/preset_recsys-min.png)

It includes 10+ metrics like NDCG at K, MAP at K, HitRate, diversity, serendipity, etc.

<Info>
  **Preset composition.** Verify the exact Preset composition at the [All Metrics reference page](/metrics/all-metrics).
</Info>

<Info>
  **Metric explainers.** Check the [Ranking and RecSys Metrics](/metrics/explainer_recsys) to see how each Metric works.
</Info>

## Use case

When to use the Preset:

* **Experimental evaluations** as you iterate on building your recommender system.

* **Side-by-side comparison** for two different models or periods.&#x20;

* **Production monitoring** checks after you acquire ground truth labels.&#x20;

## Data requirements

You must provide the following inputs:

* **Prediction.** Recommended items with rank or score.

* **Target**. True relevance score or interaction result.

* (Optional) **Input/user features**. For some diversity metrics.

* (Optional) **Training data**. For some diversity metrics.

* (Optional) **Reference dataset**. To get a side-by-side comparison.

<Info>
  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.
</Info>

## Report customization

**Metrics**. If you want to change the Metric composition, create a custom Report by simply listing the individual Metrics. You can also add custom Metrics.

<Info>
  **Creating a custom Report**. Check how to create a [Report](/docs/library/report).
</Info>

**Test conditions.**  When you enable Tests and provide a reference dataset, Evidently uses the default Test conditions based on heuristics and learned reference values. If you want to set your own pass/fail conditions, create a custom Report and add conditions to the individual Metrics.

<Info>
  **Adding Test conditions**. Check how to specify [Tests](/docs/library/tests) for your Report.
</Info>