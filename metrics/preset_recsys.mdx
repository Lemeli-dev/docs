---
title: "RecSys Preset"
description: "Overview of the Recommender Systems Preset"
noindex: "true"
---

To run a Preset on your data:

```python
report = Report(metrics=[
    RecsysPreset(k=5),
])
column_mapping = ColumnMapping(recommendations_type='rank', target='rating', prediction='rank', item_id='title', user_id='user_id')
report.run(
    reference_data=reference,
    current_data=current,
    column_mapping=column_mapping,
    additional_data={'current_train_data': train_data}
  )
report
```

To add Tests

\[ADD CODE]

## Overview

`RecsysPreset` evaluates the quality of the recommender system by generating multiple metrics to assess the quality of ranking and diversity of recommendations. You must provide the `k` parameter to evaluate the Top-K recommendations.

![](/images/metrics/preset_recsys-min.png)

It includes 10+ metrics like NDCG at K, MAP at K, HitRate, diversity, serendipity, etc. 

When to use the Preset:

* **Experimental evaluations** as you iterate on building your recommender system.

* **Side-by-side comparison** for two different models or periods.&#x20;

* **Production monitoring** checks after you acquire ground truth labels.&#x20;

<Info>
  **Preset composition.** Verify the exact Preset composition at the [All Metrics reference page](/metrics/all-metrics).
</Info>

<Info>
  **Metric explainers.** Check the [Ranking and RecSys Metrics](/metrics/explainer_recsys) to see how each Metric works.
</Info>

## Data requirements

You must provide the following inputs:

* **Prediction:** recommended items with rank or score

* **Target:** True relevance score

* (Optional) **Input/user features**: for some diversity metrics

* (Optional) **Training data**: for some diversity metrics

* (Optional) **Reference dataset**. Provide to get a side-by-side comparison.

<Info>
  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.
</Info>

## Report customization

**Metrics**. If you want to change the Metric composition, create a custom Report by simply listing the individual Metrics. You can also add custom Metrics.

<Info>
  **Creating a custom Report**. Check how to create a [Report](/docs/library/report).
</Info>

**Test conditions.**  If you enable Tests and provide a reference dataset, Evidently will use the default Test conditions dynamically learned from the reference. If you want to set your own conditions, you must create a custom Report and add conditions to the individual Metrics.

<Info>
  **Adding Test conditions**. Check how to specify [Tests](/docs/library/tests) for your Report.
</Info>